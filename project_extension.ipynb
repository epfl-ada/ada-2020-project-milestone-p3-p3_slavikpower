{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You are what you eat - Relating Demographic Data to Food Consumption Habits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns  # needed for heatmap\n",
    "from os.path import join\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.formula.api as smf\n",
    "plt.rcParams['figure.figsize'] = [7, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths for data\n",
    "FOLDER_PATH = 'data'\n",
    "PURCHASES_PATH = 'area_level_purchases'\n",
    "OSWARD_GROCERY = 'year_osward_grocery.csv'\n",
    "WARD_ATLAS = 'ward-atlas-data.csv'\n",
    "\n",
    "ward_data_path = join(FOLDER_PATH, PURCHASES_PATH, OSWARD_GROCERY)\n",
    "atlas_data_path = join(FOLDER_PATH, WARD_ATLAS)\n",
    "\n",
    "# load the data sets\n",
    "df_ward = pd.read_csv(ward_data_path, sep=',', index_col=0)\n",
    "df_atlas = pd.read_csv(atlas_data_path, sep=',', encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `df_atlas` data set looks very messy. Row 0 contains info about the data. For every ward and demographic marker yearly (from 2001 to 2006) values exist in columns. According to the [source](https://data.london.gov.uk/dataset/ward-profiles-and-atlas) this data set was collected in September 2015 with the most up-to-date information available. Thus, for many markers, the last available information dates to 2015 or even earlier (e.g. 2013). Our first step is to clean the data set from outdated columns and only keep the most up-to-date demographic markers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As indicated by the title, we are interested in demographic markers which help classifying different classes of people. These are for example gender, religion, ethnicity, wealth, age. Thus, in the following, we drop columns which we deem not helpful in classifying people into groups. We acknowledge that this process can be rather biased, however, as the data set has 946 columns, we see the need to reduce the data set's complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_redundant_data(df_atlas):\n",
    "    # clean age (columns after 2013 are projections, therefore keep 2013 columns)\n",
    "    cond = df_atlas.columns.str.startswith('Population and Age') & ~df_atlas.columns.str.contains('2013')\n",
    "    df_atlas = df_atlas.loc[:, ~cond]\n",
    "    # drop absolute population values\n",
    "    df_atlas = df_atlas.drop(['Population and Age; Population Estimates and Projections; 2013',\n",
    "                   'Population and Age; Aged 0-15; 2013',\n",
    "                   'Population and Age; Aged 16-64; 2013',\n",
    "                   'Population and Age; Aged 65+; 2013',\n",
    "                   'Population and Age; All ages; 2013',\n",
    "                   'Population and Age; Mean age; 2013',\n",
    "                   'Population and Age; Median age; 2013'], axis=1)\n",
    "    # delete population density\n",
    "    cond = df_atlas.columns.str.startswith('Area and Density')\n",
    "    df_atlas = df_atlas.loc[:, ~cond]\n",
    "    # delete the 18 ethnic groups for now, as they add complexity\n",
    "    cond = df_atlas.columns.str.startswith('Diversity; Ethnic Group 18 groups')\n",
    "    df_atlas = df_atlas.loc[:, ~cond]\n",
    "    # delete household language (of no interest) \n",
    "    cond = df_atlas.columns.str.startswith('Diversity; Household Language - 2011 Census')\n",
    "    df_atlas = df_atlas.loc[:, ~cond]\n",
    "    # delete household composition data\n",
    "    cond = df_atlas.columns.str.startswith('Household composition')\n",
    "    df_atlas = df_atlas.loc[:, ~cond]\n",
    "    # delete household repossesion\n",
    "    cond = df_atlas.columns.str.startswith('Home repossessions')\n",
    "    df_atlas = df_atlas.loc[:, ~cond]\n",
    "    # delete outdated birth and death markers\n",
    "    cond = df_atlas.columns.str.startswith('Births and deaths')\n",
    "    df_atlas = df_atlas.loc[:, ~cond]\n",
    "    # delete outdated life expectancy\n",
    "    cond = df_atlas.columns.str.startswith('Life Expectancy')\n",
    "    df_atlas = df_atlas.loc[:, ~cond]\n",
    "    # delete outdated housing markers\n",
    "    cond = df_atlas.columns.str.startswith('Housing type and Tenure')\n",
    "    df_atlas = df_atlas.loc[:, ~cond]\n",
    "    cond = df_atlas.columns.str.startswith('House Prices')\n",
    "    df_atlas = df_atlas.loc[:, ~cond]\n",
    "    # delete tax related data (not of interest)\n",
    "    cond = df_atlas.columns.str.startswith('Dwellings and Council tax')\n",
    "    df_atlas = df_atlas.loc[:, ~cond]\n",
    "    # delete dwelling data which is not of interest\n",
    "    cond = df_atlas.columns.str.startswith('Property Type') | df_atlas.columns.str.startswith('Size of dwellings') | df_atlas.columns.str.startswith('Property build period')\n",
    "    df_atlas = df_atlas.loc[:, ~cond]\n",
    "    # delete outdated income data\n",
    "    cond = df_atlas.columns.str.startswith('Household Income') & ~df_atlas.columns.str.contains('2012/13')\n",
    "    df_atlas = df_atlas.loc[:, ~cond]\n",
    "    # outdated employment data\n",
    "    cond = df_atlas.columns.str.startswith('Employment')\n",
    "    df_atlas = df_atlas.loc[:, ~cond]\n",
    "    # delete benefits claimants and jobseekers allowance\n",
    "    cond = df_atlas.columns.str.startswith('Benefits claimants') | df_atlas.columns.str.startswith('Jobseekers Allowance')\n",
    "    df_atlas = df_atlas.loc[:, ~cond]\n",
    "    # delete out of work families\n",
    "    cond = df_atlas.columns.str.startswith('Out-of-Work Families')\n",
    "    df_atlas = df_atlas.loc[:, ~cond]\n",
    "    # outdated poverty\n",
    "    cond = df_atlas.columns.str.startswith('Children in Poverty')\n",
    "    df_atlas = df_atlas.loc[:, ~cond]\n",
    "    # outdated \"intelligence\" scores\n",
    "    cond = df_atlas.columns.str.startswith('GCSE')\n",
    "    df_atlas = df_atlas.loc[:, ~cond]\n",
    "    cond = df_atlas.columns.str.startswith('A-Level Point Scores')\n",
    "    df_atlas = df_atlas.loc[:, ~cond]\n",
    "    # pupil abscence not if interest\n",
    "    cond = df_atlas.columns.str.startswith('Pupil Absence')\n",
    "    df_atlas = df_atlas.loc[:, ~cond]\n",
    "    # individual crimes not of interest, only crime rate\n",
    "    cond = df_atlas.columns.str.startswith('Crime') # & ~df_atlas.columns.str.contains('Total crime rate; 2014/15')\n",
    "    df_atlas = df_atlas.loc[:, ~cond]\n",
    "    # further columns not of interest\n",
    "    cond = df_atlas.columns.str.startswith('Fires') | df_atlas.columns.str.startswith('Ambulance') \\\n",
    "    | df_atlas.columns.str.startswith('Binge Drinking') | df_atlas.columns.str.startswith('Road Casualties') \\\n",
    "    | df_atlas.columns.str.startswith('Air Emissions') | df_atlas.columns.str.startswith('Land Use') \\\n",
    "    | df_atlas.columns.str.startswith('Access to green space and nature') | df_atlas.columns.str.startswith('Public Transport Accessibility')\\\n",
    "    | df_atlas.columns.str.startswith('Car access') | df_atlas.columns.str.startswith('Travel to work by bicycle') \\\n",
    "    | df_atlas.columns.str.startswith('Workplace employment') | df_atlas.columns.str.startswith('Nat Insurance No. registrations') \\\n",
    "    | df_atlas.columns.str.startswith('Election turnout')\n",
    "    df_atlas = df_atlas.loc[:, ~cond]\n",
    "    # obesity already available in cleaned format\n",
    "    cond = df_atlas.columns.str.startswith('Childhood Obesity Prevalence')\n",
    "    df_atlas = df_atlas.loc[:, ~cond]\n",
    "    # delete old indices of deprivation\n",
    "    cond = df_atlas.columns.str.startswith('Indices of Deprivation') & ~df_atlas.columns.str.contains('2010')\n",
    "    df_atlas = df_atlas.loc[:, ~cond]\n",
    "    cond = df_atlas.columns.str.startswith('Indices of Deprivation') & ~df_atlas.columns.str.endswith('Average Score; 2010')\n",
    "    df_atlas = df_atlas.loc[:, ~cond]\n",
    "    \n",
    "    df_atlas.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "    df_atlas.drop(df_atlas.index[-3:], inplace=True)\n",
    "    df_atlas.drop(df_atlas.index[0], inplace=True)\n",
    "    return df_atlas\n",
    "\n",
    "df_atlas = drop_redundant_data(df_atlas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first 4 columns seem to have a wrong name\n",
    "df_atlas.rename(columns={'Unnamed: 1': 'New Code', 'Unnamed: 2': 'Borough', \n",
    "                         'Unnamed: 3': 'Names'}, inplace=True)\n",
    "\n",
    "# very long column names, rename columns to more concise names\n",
    "import re\n",
    "rename_map = {name: re.sub('.*; ', '', name) for name in df_atlas.columns.values}\n",
    "# after inspection 'Household Income; Median Modelled Household income (£); 2012/13',\n",
    "# 'Household Income; Mean Modelled Household income (£); 2012/13',\n",
    "# 'Happiness and Well-being; Subjective well-being average score, 2011/12 and 2012/13; ',\n",
    "# and 'Indices of Deprivation; Average Score; 2010' have been replaced too much, manually add those column names\n",
    "rename_map['Household Income; Median Modelled Household income (£); 2012/13'] = 'Household Income Mean'\n",
    "rename_map['Household Income; Mean Modelled Household income (£); 2012/13'] = 'Household Income Median'\n",
    "rename_map['Happiness and Well-being; Subjective well-being average score, 2011/12 and 2012/13; '] = 'Well-Being'\n",
    "rename_map['Indices of Deprivation; Average Score; 2010'] = 'IOD AVG'\n",
    "df_atlas.rename(columns=rename_map, inplace=True)\n",
    "                         \n",
    "df_atlas.rename(columns={'% No qualifications': 'No qualifications', \n",
    "                         '% Highest level of qualification: Level 1 qualifications': 'Level 1 qualifications', \n",
    "                         '% Highest level of qualification: Level 2 qualifications': 'Level 2 qualifications', \n",
    "                         '% Highest level of qualification: Apprenticeship': 'Apprenticeship qualifications', \n",
    "                         '% Highest level of qualification: Level 3 qualifications': 'Level 3 qualifications', \n",
    "                         '% Highest level of qualification: Level 4 qualifications and above': 'Level >=4 qualifications',\n",
    "                         '% Highest level of qualification: Other qualifications': 'Other qualifications', \n",
    "                         '% People with Bad or Very Bad Health': 'Bad Health',\n",
    "                         '% People whose Day-to-day activities are limited a lot': 'Limited activities'},\n",
    "                inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to numeric conversion\n",
    "df_atlas.loc[:, 'Aged 0-15':] = df_atlas.loc[:, 'Aged 0-15':].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_cols = ['Aged 0-15', 'Aged 16-64', 'Aged 65+'] #sum to 1\n",
    "ethnicities_cols = ['White', 'Mixed', 'Asian or Asian British', 'Black or Black British', 'Other'] #sum to 1\n",
    "religion_cols = ['Christian', 'Buddhist', 'Hindu', 'Jewish', 'Muslim', 'Sikh',\n",
    "       'No religion', 'Other religions', 'Religion not stated'] # sum to 1\n",
    "born_cols = ['Born in UK', 'Not Born in UK'] # sum to 1\n",
    "wealth_cols = ['Household Income Median', 'IOD AVG'] #'Well-Being'\n",
    "qualilication_cols = ['No qualifications', 'Level 1 qualifications', 'Level 2 qualifications',\n",
    "       'Apprenticeship qualifications', 'Level 3 qualifications',\n",
    "       'Level >=4 qualifications', 'Other qualifications'] # sum to 1 \n",
    "disability_cols = ['Bad Health', 'Limited activities']\n",
    "all_features = age_cols + ethnicities_cols + religion_cols + born_cols + wealth_cols + qualilication_cols + disability_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df, cols):\n",
    "    total = df[cols].sum(axis=1)\n",
    "    df[cols] = df[cols].div(total, axis=0)\n",
    "    return df\n",
    "\n",
    "def standartize(df, cols):\n",
    "    means = df[cols].mean()\n",
    "    stds = df[cols].std()\n",
    "    df[cols] = (df[cols] - means) / stds\n",
    "    return df\n",
    "\n",
    "def min_max_scaling(df, cols):\n",
    "    mins = df[cols].min()\n",
    "    maxs = df[cols].max()\n",
    "    df[cols] = (df[cols] - mins) / (maxs - mins)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize\n",
    "df_atlas[age_cols] /= 100\n",
    "df_atlas = normalize(df_atlas, ethnicities_cols)\n",
    "df_atlas = normalize(df_atlas, religion_cols)\n",
    "df_atlas = normalize(df_atlas, born_cols)\n",
    "df_atlas[qualilication_cols] /= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for cols in [age_cols, ethnicities_cols, religion_cols, born_cols, qualilication_cols]:\n",
    "    print(any(np.abs(df_atlas[cols].sum(axis=1) - 1) > 0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "df_atlas['Household Income Median'].hist(bins=50, ax=ax[0])\n",
    "(np.log(df_atlas['Household Income Median'])).hist(bins=50, ax=ax[1])\n",
    "plt.show()\n",
    "#haivy-tailed, makes sense to apply log transform before min-max rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "df_atlas['IOD AVG'].hist(bins=50, ax=ax[0])\n",
    "(np.log(df_atlas['IOD AVG'])).hist(bins=50, ax=ax[1])\n",
    "plt.show()\n",
    "#no sense to apply log transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atlas['Household Income Median'] = df_atlas['Household Income Median'].apply(np.log)\n",
    "df_atlas = standartize(df_atlas, wealth_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "df_atlas['Bad Health'].hist(bins=50, ax=ax[0])\n",
    "(np.log(df_atlas['Bad Health'])).hist(bins=50, ax=ax[1])\n",
    "plt.show()\n",
    "#no sense to apply log transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "df_atlas['Limited activities'].hist(bins=50, ax=ax[0])\n",
    "(np.log(df_atlas['Limited activities'])).hist(bins=50, ax=ax[1])\n",
    "plt.show()\n",
    "#no sense to apply log transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atlas = standartize(df_atlas, disability_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The indices of deprivation measure how deprived an area is [source](https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/464597/English_Indices_of_Deprivation_2015_-_Research_Report.pdf). In essence, these indicators indicate how impoverished an area is. The linked documents lists the meaning of the different indices. *The average score measure summarises the average level of deprivation across the higher-level area,based on the scores of the Lower-layer Super Output Areas in the area.* Therefore, we decided to keep only this measure, as we are interested in an indicator of \"average impoverishment\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['weight', 'weight_perc2.5', 'weight_perc25', 'weight_perc50',\n",
       "       'weight_perc75', 'weight_perc97.5', 'weight_std', 'weight_ci95',\n",
       "       'volume', 'volume_perc2.5', 'volume_perc25', 'volume_perc50',\n",
       "       'volume_perc75', 'volume_perc97.5', 'volume_std', 'volume_ci95',\n",
       "       'fat', 'fat_perc2.5', 'fat_perc25', 'fat_perc50', 'fat_perc75',\n",
       "       'fat_perc97.5', 'fat_std', 'fat_ci95', 'saturate',\n",
       "       'saturate_perc2.5', 'saturate_perc25', 'saturate_perc50',\n",
       "       'saturate_perc75', 'saturate_perc97.5', 'saturate_std',\n",
       "       'saturate_ci95', 'salt', 'salt_perc2.5', 'salt_perc25',\n",
       "       'salt_perc50', 'salt_perc75', 'salt_perc97.5', 'salt_std',\n",
       "       'salt_ci95', 'sugar', 'sugar_perc2.5', 'sugar_perc25',\n",
       "       'sugar_perc50', 'sugar_perc75', 'sugar_perc97.5', 'sugar_std',\n",
       "       'sugar_ci95', 'protein', 'protein_perc2.5', 'protein_perc25',\n",
       "       'protein_perc50', 'protein_perc75', 'protein_perc97.5',\n",
       "       'protein_std', 'protein_ci95', 'carb', 'carb_perc2.5',\n",
       "       'carb_perc25', 'carb_perc50', 'carb_perc75', 'carb_perc97.5',\n",
       "       'carb_std', 'carb_ci95', 'fibre', 'fibre_perc2.5', 'fibre_perc25',\n",
       "       'fibre_perc50', 'fibre_perc75', 'fibre_perc97.5', 'fibre_std',\n",
       "       'fibre_ci95', 'alcohol', 'alcohol_perc2.5', 'alcohol_perc25',\n",
       "       'alcohol_perc50', 'alcohol_perc75', 'alcohol_perc97.5',\n",
       "       'alcohol_std', 'alcohol_ci95', 'energy_fat', 'energy_fat_perc2.5',\n",
       "       'energy_fat_perc25', 'energy_fat_perc50', 'energy_fat_perc75',\n",
       "       'energy_fat_perc97.5', 'energy_fat_std', 'energy_fat_ci95',\n",
       "       'energy_saturate', 'energy_saturate_perc2.5',\n",
       "       'energy_saturate_perc25', 'energy_saturate_perc50',\n",
       "       'energy_saturate_perc75', 'energy_saturate_perc97.5',\n",
       "       'energy_saturate_std', 'energy_saturate_ci95', 'energy_sugar',\n",
       "       'energy_sugar_perc2.5', 'energy_sugar_perc25',\n",
       "       'energy_sugar_perc50', 'energy_sugar_perc75',\n",
       "       'energy_sugar_perc97.5', 'energy_sugar_std', 'energy_sugar_ci95',\n",
       "       'energy_protein', 'energy_protein_perc2.5',\n",
       "       'energy_protein_perc25', 'energy_protein_perc50',\n",
       "       'energy_protein_perc75', 'energy_protein_perc97.5',\n",
       "       'energy_protein_std', 'energy_protein_ci95', 'energy_carb',\n",
       "       'energy_carb_perc2.5', 'energy_carb_perc25', 'energy_carb_perc50',\n",
       "       'energy_carb_perc75', 'energy_carb_perc97.5', 'energy_carb_std',\n",
       "       'energy_carb_ci95', 'energy_fibre', 'energy_fibre_perc2.5',\n",
       "       'energy_fibre_perc25', 'energy_fibre_perc50',\n",
       "       'energy_fibre_perc75', 'energy_fibre_perc97.5', 'energy_fibre_std',\n",
       "       'energy_fibre_ci95', 'energy_alcohol', 'energy_alcohol_perc2.5',\n",
       "       'energy_alcohol_perc25', 'energy_alcohol_perc50',\n",
       "       'energy_alcohol_perc75', 'energy_alcohol_perc97.5',\n",
       "       'energy_alcohol_std', 'energy_alcohol_ci95', 'energy_tot',\n",
       "       'energy_tot_perc2.5', 'energy_tot_perc25', 'energy_tot_perc50',\n",
       "       'energy_tot_perc75', 'energy_tot_perc97.5', 'energy_tot_std',\n",
       "       'energy_tot_ci95', 'f_energy_fat', 'f_energy_saturate',\n",
       "       'f_energy_sugar', 'f_energy_protein', 'f_energy_carb',\n",
       "       'f_energy_fibre', 'f_energy_alcohol', 'energy_density',\n",
       "       'h_nutrients_weight', 'h_nutrients_weight_norm',\n",
       "       'h_nutrients_calories', 'h_nutrients_calories_norm', 'f_beer',\n",
       "       'f_dairy', 'f_eggs', 'f_fats_oils', 'f_fish', 'f_fruit_veg',\n",
       "       'f_grains', 'f_meat_red', 'f_poultry', 'f_readymade', 'f_sauces',\n",
       "       'f_soft_drinks', 'f_spirits', 'f_sweets', 'f_tea_coffee',\n",
       "       'f_water', 'f_wine', 'f_dairy_weight', 'f_eggs_weight',\n",
       "       'f_fats_oils_weight', 'f_fish_weight', 'f_fruit_veg_weight',\n",
       "       'f_grains_weight', 'f_meat_red_weight', 'f_poultry_weight',\n",
       "       'f_readymade_weight', 'f_sauces_weight', 'f_sweets_weight',\n",
       "       'h_items', 'h_items_norm', 'h_items_weight', 'h_items_weight_norm',\n",
       "       'representativeness_norm', 'transaction_days', 'num_transactions',\n",
       "       'man_day', 'population', 'male', 'female', 'age_0_17', 'age_18_64',\n",
       "       'age_65+', 'avg_age', 'area_sq_km', 'people_per_sq_km'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ward.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_cols = ['weight', 'volume', 'fat', 'saturate', 'salt', 'sugar', 'protein', 'carb',\n",
    "                 'fibre', 'alcohol', 'energy_fat', 'energy_saturate', 'energy_sugar', 'energy_protein',\n",
    "                 'energy_carb', 'energy_fibre', 'energy_alcohol', 'energy_tot', 'f_energy_fat', 'f_energy_saturate', \n",
    "                 'f_energy_sugar', 'f_energy_protein', 'f_energy_carb', 'f_energy_fibre', 'f_energy_alcohol', \n",
    "                 'energy_density', 'h_nutrients_weight', 'h_nutrients_weight_norm', 'h_nutrients_calories',\n",
    "                  'h_nutrients_calories_norm', 'representativeness_norm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before merging set index of df_atlas to the ward code\n",
    "df_atlas.set_index('New Code', drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight</th>\n",
       "      <th>volume</th>\n",
       "      <th>fat</th>\n",
       "      <th>saturate</th>\n",
       "      <th>salt</th>\n",
       "      <th>sugar</th>\n",
       "      <th>protein</th>\n",
       "      <th>carb</th>\n",
       "      <th>fibre</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>...</th>\n",
       "      <th>Level 2 qualifications</th>\n",
       "      <th>Apprenticeship qualifications</th>\n",
       "      <th>Level 3 qualifications</th>\n",
       "      <th>Level &gt;=4 qualifications</th>\n",
       "      <th>Other qualifications</th>\n",
       "      <th>% of 16+ who are schoolchildren and full-time students: Age 18 and over</th>\n",
       "      <th>Bad Health</th>\n",
       "      <th>Limited activities</th>\n",
       "      <th>Well-Being</th>\n",
       "      <th>IOD AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>E05000026</th>\n",
       "      <td>449.535137</td>\n",
       "      <td>125.960833</td>\n",
       "      <td>9.488797</td>\n",
       "      <td>3.693721</td>\n",
       "      <td>0.583240</td>\n",
       "      <td>10.966213</td>\n",
       "      <td>4.977560</td>\n",
       "      <td>19.381951</td>\n",
       "      <td>1.564721</td>\n",
       "      <td>0.198172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.167</td>\n",
       "      <td>14.3</td>\n",
       "      <td>-0.492858</td>\n",
       "      <td>-1.121216</td>\n",
       "      <td>7.897959</td>\n",
       "      <td>0.694138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E05000027</th>\n",
       "      <td>413.130263</td>\n",
       "      <td>110.664114</td>\n",
       "      <td>9.733634</td>\n",
       "      <td>3.565913</td>\n",
       "      <td>0.568184</td>\n",
       "      <td>10.514427</td>\n",
       "      <td>5.211694</td>\n",
       "      <td>18.950348</td>\n",
       "      <td>1.581960</td>\n",
       "      <td>0.209917</td>\n",
       "      <td>...</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.089</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.409580</td>\n",
       "      <td>1.429760</td>\n",
       "      <td>7.358842</td>\n",
       "      <td>0.992115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E05000028</th>\n",
       "      <td>407.100472</td>\n",
       "      <td>121.990710</td>\n",
       "      <td>9.216310</td>\n",
       "      <td>3.613582</td>\n",
       "      <td>0.610536</td>\n",
       "      <td>10.690272</td>\n",
       "      <td>5.192412</td>\n",
       "      <td>19.662048</td>\n",
       "      <td>1.572323</td>\n",
       "      <td>0.225425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.100</td>\n",
       "      <td>7.7</td>\n",
       "      <td>1.647385</td>\n",
       "      <td>1.429760</td>\n",
       "      <td>7.609123</td>\n",
       "      <td>0.601395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E05000029</th>\n",
       "      <td>384.173858</td>\n",
       "      <td>122.245578</td>\n",
       "      <td>9.700137</td>\n",
       "      <td>3.961264</td>\n",
       "      <td>0.533180</td>\n",
       "      <td>12.938606</td>\n",
       "      <td>4.718184</td>\n",
       "      <td>20.084734</td>\n",
       "      <td>1.550344</td>\n",
       "      <td>0.200380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.083</td>\n",
       "      <td>5.3</td>\n",
       "      <td>1.488848</td>\n",
       "      <td>2.119213</td>\n",
       "      <td>7.108108</td>\n",
       "      <td>1.168602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E05000030</th>\n",
       "      <td>356.882607</td>\n",
       "      <td>109.959688</td>\n",
       "      <td>9.381808</td>\n",
       "      <td>3.614663</td>\n",
       "      <td>0.566784</td>\n",
       "      <td>11.332898</td>\n",
       "      <td>5.307003</td>\n",
       "      <td>19.581403</td>\n",
       "      <td>1.607947</td>\n",
       "      <td>0.168952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.061</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.775434</td>\n",
       "      <td>1.291869</td>\n",
       "      <td>7.887917</td>\n",
       "      <td>0.065051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E05000645</th>\n",
       "      <td>387.651694</td>\n",
       "      <td>100.374890</td>\n",
       "      <td>9.011954</td>\n",
       "      <td>3.558849</td>\n",
       "      <td>0.584727</td>\n",
       "      <td>9.403091</td>\n",
       "      <td>5.499603</td>\n",
       "      <td>16.729870</td>\n",
       "      <td>1.609476</td>\n",
       "      <td>0.292334</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.099</td>\n",
       "      <td>6.4</td>\n",
       "      <td>0.616897</td>\n",
       "      <td>0.257690</td>\n",
       "      <td>7.653520</td>\n",
       "      <td>-0.504062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E05000646</th>\n",
       "      <td>391.296517</td>\n",
       "      <td>102.209867</td>\n",
       "      <td>8.810350</td>\n",
       "      <td>3.379945</td>\n",
       "      <td>0.604999</td>\n",
       "      <td>9.350598</td>\n",
       "      <td>5.584789</td>\n",
       "      <td>17.240961</td>\n",
       "      <td>1.640990</td>\n",
       "      <td>0.209072</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.104</td>\n",
       "      <td>7.7</td>\n",
       "      <td>0.537629</td>\n",
       "      <td>0.188745</td>\n",
       "      <td>7.578400</td>\n",
       "      <td>-0.262902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E05000647</th>\n",
       "      <td>391.146713</td>\n",
       "      <td>115.080184</td>\n",
       "      <td>8.642757</td>\n",
       "      <td>3.398100</td>\n",
       "      <td>0.584833</td>\n",
       "      <td>9.446028</td>\n",
       "      <td>5.321004</td>\n",
       "      <td>17.579308</td>\n",
       "      <td>1.680360</td>\n",
       "      <td>0.299310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.097</td>\n",
       "      <td>7.7</td>\n",
       "      <td>0.220556</td>\n",
       "      <td>-0.500708</td>\n",
       "      <td>8.185989</td>\n",
       "      <td>-0.452034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E05000648</th>\n",
       "      <td>400.832576</td>\n",
       "      <td>109.727023</td>\n",
       "      <td>8.919674</td>\n",
       "      <td>3.512147</td>\n",
       "      <td>0.549463</td>\n",
       "      <td>9.623928</td>\n",
       "      <td>5.259262</td>\n",
       "      <td>17.940815</td>\n",
       "      <td>1.611090</td>\n",
       "      <td>0.195706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.131</td>\n",
       "      <td>7.4</td>\n",
       "      <td>2.994945</td>\n",
       "      <td>2.739721</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.459673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E05000649</th>\n",
       "      <td>370.849508</td>\n",
       "      <td>99.419205</td>\n",
       "      <td>9.154919</td>\n",
       "      <td>3.642411</td>\n",
       "      <td>0.560853</td>\n",
       "      <td>10.170574</td>\n",
       "      <td>5.328714</td>\n",
       "      <td>17.765000</td>\n",
       "      <td>1.665483</td>\n",
       "      <td>0.217665</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.134</td>\n",
       "      <td>10.1</td>\n",
       "      <td>-0.334322</td>\n",
       "      <td>-0.983325</td>\n",
       "      <td>7.731481</td>\n",
       "      <td>-0.390090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>483 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               weight      volume       fat  saturate      salt      sugar  \\\n",
       "E05000026  449.535137  125.960833  9.488797  3.693721  0.583240  10.966213   \n",
       "E05000027  413.130263  110.664114  9.733634  3.565913  0.568184  10.514427   \n",
       "E05000028  407.100472  121.990710  9.216310  3.613582  0.610536  10.690272   \n",
       "E05000029  384.173858  122.245578  9.700137  3.961264  0.533180  12.938606   \n",
       "E05000030  356.882607  109.959688  9.381808  3.614663  0.566784  11.332898   \n",
       "...               ...         ...       ...       ...       ...        ...   \n",
       "E05000645  387.651694  100.374890  9.011954  3.558849  0.584727   9.403091   \n",
       "E05000646  391.296517  102.209867  8.810350  3.379945  0.604999   9.350598   \n",
       "E05000647  391.146713  115.080184  8.642757  3.398100  0.584833   9.446028   \n",
       "E05000648  400.832576  109.727023  8.919674  3.512147  0.549463   9.623928   \n",
       "E05000649  370.849508   99.419205  9.154919  3.642411  0.560853  10.170574   \n",
       "\n",
       "            protein       carb     fibre   alcohol  ...  \\\n",
       "E05000026  4.977560  19.381951  1.564721  0.198172  ...   \n",
       "E05000027  5.211694  18.950348  1.581960  0.209917  ...   \n",
       "E05000028  5.192412  19.662048  1.572323  0.225425  ...   \n",
       "E05000029  4.718184  20.084734  1.550344  0.200380  ...   \n",
       "E05000030  5.307003  19.581403  1.607947  0.168952  ...   \n",
       "...             ...        ...       ...       ...  ...   \n",
       "E05000645  5.499603  16.729870  1.609476  0.292334  ...   \n",
       "E05000646  5.584789  17.240961  1.640990  0.209072  ...   \n",
       "E05000647  5.321004  17.579308  1.680360  0.299310  ...   \n",
       "E05000648  5.259262  17.940815  1.611090  0.195706  ...   \n",
       "E05000649  5.328714  17.765000  1.665483  0.217665  ...   \n",
       "\n",
       "           Level 2 qualifications  Apprenticeship qualifications  \\\n",
       "E05000026                   0.115                          0.011   \n",
       "E05000027                   0.157                          0.018   \n",
       "E05000028                   0.153                          0.020   \n",
       "E05000029                   0.149                          0.022   \n",
       "E05000030                   0.165                          0.028   \n",
       "...                           ...                            ...   \n",
       "E05000645                   0.076                          0.009   \n",
       "E05000646                   0.081                          0.009   \n",
       "E05000647                   0.071                          0.010   \n",
       "E05000648                   0.096                          0.008   \n",
       "E05000649                   0.068                          0.006   \n",
       "\n",
       "           Level 3 qualifications  Level >=4 qualifications  \\\n",
       "E05000026                   0.085                     0.345   \n",
       "E05000027                   0.093                     0.167   \n",
       "E05000028                   0.091                     0.206   \n",
       "E05000029                   0.100                     0.195   \n",
       "E05000030                   0.100                     0.185   \n",
       "...                           ...                       ...   \n",
       "E05000645                   0.080                     0.551   \n",
       "E05000646                   0.088                     0.531   \n",
       "E05000647                   0.085                     0.585   \n",
       "E05000648                   0.089                     0.357   \n",
       "E05000649                   0.095                     0.546   \n",
       "\n",
       "           Other qualifications  \\\n",
       "E05000026                 0.167   \n",
       "E05000027                 0.089   \n",
       "E05000028                 0.100   \n",
       "E05000029                 0.083   \n",
       "E05000030                 0.061   \n",
       "...                         ...   \n",
       "E05000645                 0.099   \n",
       "E05000646                 0.104   \n",
       "E05000647                 0.097   \n",
       "E05000648                 0.131   \n",
       "E05000649                 0.134   \n",
       "\n",
       "           % of 16+ who are schoolchildren and full-time students: Age 18 and over  \\\n",
       "E05000026                                               14.3                         \n",
       "E05000027                                                5.8                         \n",
       "E05000028                                                7.7                         \n",
       "E05000029                                                5.3                         \n",
       "E05000030                                                5.0                         \n",
       "...                                                      ...                         \n",
       "E05000645                                                6.4                         \n",
       "E05000646                                                7.7                         \n",
       "E05000647                                                7.7                         \n",
       "E05000648                                                7.4                         \n",
       "E05000649                                               10.1                         \n",
       "\n",
       "           Bad Health  Limited activities  Well-Being   IOD AVG  \n",
       "E05000026   -0.492858           -1.121216    7.897959  0.694138  \n",
       "E05000027    1.409580            1.429760    7.358842  0.992115  \n",
       "E05000028    1.647385            1.429760    7.609123  0.601395  \n",
       "E05000029    1.488848            2.119213    7.108108  1.168602  \n",
       "E05000030    0.775434            1.291869    7.887917  0.065051  \n",
       "...               ...                 ...         ...       ...  \n",
       "E05000645    0.616897            0.257690    7.653520 -0.504062  \n",
       "E05000646    0.537629            0.188745    7.578400 -0.262902  \n",
       "E05000647    0.220556           -0.500708    8.185989 -0.452034  \n",
       "E05000648    2.994945            2.739721    8.000000  1.459673  \n",
       "E05000649   -0.334322           -0.983325    7.731481 -0.390090  \n",
       "\n",
       "[483 rows x 66 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge the data sets by ward\n",
    "df_ward_atlas = pd.merge(df_ward[important_cols], df_atlas, how='inner', left_index=True, right_index=True, validate='1:1')\n",
    "df_ward_atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Level 1 qualifications           0.824816\n",
       "Level >=4 qualifications        -0.787306\n",
       "Level 2 qualifications           0.772630\n",
       "No qualifications                0.686948\n",
       "Apprenticeship qualifications    0.643280\n",
       "Household Income Median         -0.622505\n",
       "Aged 16-64                      -0.575291\n",
       "Aged 0-15                        0.499667\n",
       "Religion not stated             -0.476486\n",
       "Mixed                           -0.413051\n",
       "Limited activities               0.382785\n",
       "No religion                     -0.374852\n",
       "Buddhist                        -0.343616\n",
       "Other                           -0.307474\n",
       "Jewish                          -0.305896\n",
       "Not Born in UK                  -0.300743\n",
       "Born in UK                       0.300743\n",
       "Aged 65+                         0.260227\n",
       "Asian or Asian British           0.249848\n",
       "Hindu                            0.233352\n",
       "Sikh                             0.232523\n",
       "Christian                        0.206357\n",
       "Bad Health                       0.202186\n",
       "White                           -0.160661\n",
       "Level 3 qualifications           0.157957\n",
       "Muslim                           0.134663\n",
       "Black or Black British           0.075192\n",
       "IOD AVG                          0.054705\n",
       "Other religions                 -0.046007\n",
       "Other qualifications            -0.031801\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carb_energy_corr = df_ward_atlas[all_features].corrwith(df_ward_atlas['energy_carb'], method='pearson')\n",
    "carb_energy_corr = carb_energy_corr.sort_values(ascending=False, key=np.abs)\n",
    "carb_energy_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rid of linearly dependent cols\n",
    "for cols in [age_cols, ethnicities_cols, religion_cols, born_cols, qualilication_cols]:\n",
    "    carb_energy_corr.drop(carb_energy_corr[cols].index[-1], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Level 1 qualifications           0.824816\n",
       "Level >=4 qualifications        -0.787306\n",
       "Level 2 qualifications           0.772630\n",
       "No qualifications                0.686948\n",
       "Apprenticeship qualifications    0.643280\n",
       "Household Income Median         -0.622505\n",
       "Aged 16-64                      -0.575291\n",
       "Aged 0-15                        0.499667\n",
       "Mixed                           -0.413051\n",
       "Limited activities               0.382785\n",
       "No religion                     -0.374852\n",
       "Buddhist                        -0.343616\n",
       "Jewish                          -0.305896\n",
       "Born in UK                       0.300743\n",
       "Asian or Asian British           0.249848\n",
       "Hindu                            0.233352\n",
       "Sikh                             0.232523\n",
       "Christian                        0.206357\n",
       "Bad Health                       0.202186\n",
       "White                           -0.160661\n",
       "Level 3 qualifications           0.157957\n",
       "Muslim                           0.134663\n",
       "Black or Black British           0.075192\n",
       "IOD AVG                          0.054705\n",
       "Other religions                 -0.046007\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carb_energy_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_feature_selection(df_corrs, target_col, verbose=False):\n",
    "\n",
    "    num_features = 1\n",
    "    repeat = 100\n",
    "    best_score = 0\n",
    "    best_num_features = 0\n",
    "    delay = 10\n",
    "    not_improvements = 0\n",
    "\n",
    "    while (not_improvements < delay) and (num_features < len(df_corrs)):\n",
    "        features = list(df_corrs[:num_features].keys())\n",
    "        x_data = df_ward_atlas[features]\n",
    "        y_data = df_ward_atlas[target_col]\n",
    "        res_train = []\n",
    "        res_test = []\n",
    "        for i in range(repeat):\n",
    "            x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 1/3)\n",
    "            model = LinearRegression()\n",
    "            model.fit(x_train, y_train)\n",
    "            res_train.append(model.score(x_train, y_train))\n",
    "            res_test.append(model.score(x_test, y_test))\n",
    "        res_train = np.array(res_train)\n",
    "        res_test = np.array(res_test)\n",
    "        res_train = np.mean(res_train)\n",
    "        res_test = np.mean(res_test)\n",
    "        if (res_test > best_score):\n",
    "            summary_str = 'R2 score on test data improved by {:.5}'.format(res_test-best_score)\n",
    "            best_score = res_test\n",
    "            best_num_features = num_features\n",
    "            not_improvements = 0\n",
    "        else:\n",
    "            summary_str = 'R2 score on test data did not improved'\n",
    "            not_improvements += 1\n",
    "        if verbose:\n",
    "            print('Added feature {}: {}'.format(num_features, df_corrs.keys()[num_features-1]))\n",
    "            print('Average r2_score on train data: {:.5}, on test data {:.5}'.format(res_train, res_test))\n",
    "            print(summary_str + '\\n')\n",
    "        num_features += 1\n",
    "    \n",
    "    return best_score, best_num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_feature_selection_brute_force(df_corrs_, target_col, verbose=False):\n",
    "    num_features = 1\n",
    "    repeat = 100\n",
    "    global_test_best = 0\n",
    "    selected_cols_best = None\n",
    "    delay = 3\n",
    "    not_improvements = 0\n",
    "\n",
    "    df_corrs = df_corrs_.copy()\n",
    "    selected_cols = []\n",
    "\n",
    "    while (not_improvements < delay) and (len(df_corrs) > 0):\n",
    "        res_train = {}\n",
    "        res_test = {}\n",
    "        local_train_best = 0\n",
    "        local_test_best = 0\n",
    "        best_new_col = None\n",
    "        for new_col in df_corrs.index:\n",
    "            trial_cols = selected_cols + [new_col]       \n",
    "            x_data = df_ward_atlas[trial_cols]\n",
    "            y_data = df_ward_atlas[target_col]\n",
    "            res_train[new_col] = []\n",
    "            res_test[new_col] = []\n",
    "            for i in range(repeat):\n",
    "                x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 1/3)\n",
    "                model = LinearRegression()\n",
    "                model.fit(x_train, y_train)\n",
    "                res_train[new_col].append(model.score(x_train, y_train))\n",
    "                res_test[new_col].append(model.score(x_test, y_test))\n",
    "            res_train[new_col] = np.array(res_train[new_col])\n",
    "            res_test[new_col] = np.array(res_test[new_col])\n",
    "            res_train[new_col] = np.mean(res_train[new_col])\n",
    "            res_test[new_col] = np.mean(res_test[new_col])\n",
    "            if (res_test[new_col] > local_test_best):\n",
    "                local_test_best = res_test[new_col]\n",
    "                local_train_best = res_train[new_col]\n",
    "                best_new_col = new_col\n",
    "        \n",
    "        selected_cols.append(best_new_col)\n",
    "        df_corrs.drop(best_new_col, axis=0, inplace=True)\n",
    "        if (local_test_best > global_test_best):\n",
    "            summary_str = 'R2 score on test data improved by {:.5}'.format(local_test_best-global_test_best)\n",
    "            global_test_best = local_test_best\n",
    "            selected_cols_best = selected_cols.copy()\n",
    "            not_improvements = 0\n",
    "        else:\n",
    "            summary_str = 'R2 score on test data did not improved'\n",
    "            not_improvements += 1\n",
    "        if verbose:\n",
    "            print('Added feature {}: {}'.format(num_features, best_new_col))\n",
    "            print('Average r2_score on train data: {:.5}, on test data {:.5}'.format(local_train_best, local_test_best))\n",
    "            print(summary_str + '\\n')\n",
    "        num_features += 1\n",
    "        \n",
    "    return global_test_best, selected_cols_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added feature 1: Level 1 qualifications\n",
      "Average r2_score on train data: 0.68185, on test data 0.6715\n",
      "R2 score on test data improved by 0.6715\n",
      "\n",
      "Added feature 2: Mixed\n",
      "Average r2_score on train data: 0.71433, on test data 0.70443\n",
      "R2 score on test data improved by 0.032934\n",
      "\n",
      "Added feature 3: Jewish\n",
      "Average r2_score on train data: 0.73438, on test data 0.72843\n",
      "R2 score on test data improved by 0.023998\n",
      "\n",
      "Added feature 4: Asian or Asian British\n",
      "Average r2_score on train data: 0.75126, on test data 0.74344\n",
      "R2 score on test data improved by 0.015014\n",
      "\n",
      "Added feature 5: Level 2 qualifications\n",
      "Average r2_score on train data: 0.76619, on test data 0.74868\n",
      "R2 score on test data improved by 0.0052364\n",
      "\n",
      "Added feature 6: Muslim\n",
      "Average r2_score on train data: 0.76563, on test data 0.75919\n",
      "R2 score on test data improved by 0.010511\n",
      "\n",
      "Added feature 7: No religion\n",
      "Average r2_score on train data: 0.77378, on test data 0.76783\n",
      "R2 score on test data improved by 0.0086428\n",
      "\n",
      "Added feature 8: Level >=4 qualifications\n",
      "Average r2_score on train data: 0.77367, on test data 0.77084\n",
      "R2 score on test data improved by 0.0030089\n",
      "\n",
      "Added feature 9: Aged 16-64\n",
      "Average r2_score on train data: 0.77702, on test data 0.76771\n",
      "R2 score on test data did not improved\n",
      "\n",
      "Added feature 10: Apprenticeship qualifications\n",
      "Average r2_score on train data: 0.77656, on test data 0.76812\n",
      "R2 score on test data did not improved\n",
      "\n",
      "Added feature 11: Aged 0-15\n",
      "Average r2_score on train data: 0.77802, on test data 0.7667\n",
      "R2 score on test data did not improved\n",
      "\n"
     ]
    }
   ],
   "source": [
    "global_test_best, selected_cols_best  = forward_feature_selection_brute_force(carb_energy_corr, 'energy_carb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_test_best, selected_cols_best  = forward_feature_selection_brute_force(carb_energy_corr, 'energy_carb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7708398809492684\n",
      "['Level 1 qualifications', 'Mixed', 'Jewish', 'Asian or Asian British', 'Level 2 qualifications', 'Muslim', 'No religion', 'Level >=4 qualifications']\n"
     ]
    }
   ],
   "source": [
    "print(global_test_best)\n",
    "print(selected_cols_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "energy_fat: best r2 score = 0.34637, num features = 10\n",
      "energy_saturate: best r2 score = 0.34950, num features = 11\n",
      "energy_sugar: best r2 score = 0.59490, num features = 13\n",
      "energy_protein: best r2 score = 0.61036, num features = 15\n",
      "energy_carb: best r2 score = 0.76988, num features = 9\n",
      "energy_fibre: best r2 score = 0.58866, num features = 10\n",
      "energy_alcohol: best r2 score = 0.39419, num features = 8\n",
      "energy_tot: best r2 score = 0.65720, num features = 9\n"
     ]
    }
   ],
   "source": [
    "target_cols = ['energy_fat', 'energy_saturate', 'energy_sugar', 'energy_protein',\n",
    "                 'energy_carb', 'energy_fibre', 'energy_alcohol', 'energy_tot']\n",
    "global_test_best = {}\n",
    "selected_cols_best = {}\n",
    "energy_corr = {}\n",
    "for y in target_cols:\n",
    "    energy_corr[y] = df_ward_atlas[all_features].corrwith(df_ward_atlas[y], method='pearson')\n",
    "    energy_corr[y] = energy_corr[y].sort_values(ascending=False, key=np.abs)\n",
    "    #get rid of linearly dependent cols\n",
    "    for cols in [age_cols, ethnicities_cols, religion_cols, born_cols, qualilication_cols]:\n",
    "        energy_corr[y].drop(energy_corr[y][cols].index[-1], axis=0, inplace=True)\n",
    "    global_test_best[y], selected_cols_best[y]  = forward_feature_selection_brute_force(energy_corr[y], y)\n",
    "    print('{}: best r2 score = {:.5f}, num features = {}'.format(y, global_test_best[y], len(selected_cols_best[y])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features for 'energy_fat' prediction: ['Level 1 qualifications', 'Jewish', 'Mixed', 'Aged 0-15', 'Christian', 'Black or Black British', 'Muslim', 'No religion', 'Asian or Asian British', 'IOD AVG']\n",
      "features for 'energy_saturate' prediction: ['Asian or Asian British', 'Born in UK', 'Muslim', 'White', 'Jewish', 'Aged 0-15', 'Mixed', 'Apprenticeship qualifications', 'Level 1 qualifications', 'Level 3 qualifications', 'Other religions']\n",
      "features for 'energy_sugar' prediction: ['Level 1 qualifications', 'Black or Black British', 'Bad Health', 'White', 'Hindu', 'Mixed', 'Asian or Asian British', 'No religion', 'Aged 0-15', 'Christian', 'Muslim', 'IOD AVG', 'Apprenticeship qualifications']\n",
      "features for 'energy_protein' prediction: ['Asian or Asian British', 'Jewish', 'Level 1 qualifications', 'Sikh', 'Muslim', 'No religion', 'Level 2 qualifications', 'Christian', 'IOD AVG', 'Household Income Median', 'Apprenticeship qualifications', 'Buddhist', 'Level >=4 qualifications', 'Born in UK', 'Aged 16-64']\n",
      "features for 'energy_carb' prediction: ['Level 1 qualifications', 'Mixed', 'Jewish', 'White', 'Born in UK', 'Other religions', 'Limited activities', 'Level >=4 qualifications', 'No qualifications']\n",
      "features for 'energy_fibre' prediction: ['IOD AVG', 'No religion', 'Level 1 qualifications', 'Sikh', 'White', 'Aged 16-64', 'Household Income Median', 'Jewish', 'No qualifications', 'Black or Black British']\n",
      "features for 'energy_alcohol' prediction: ['Level >=4 qualifications', 'No religion', 'Jewish', 'Muslim', 'Apprenticeship qualifications', 'Other religions', 'IOD AVG', 'Bad Health']\n",
      "features for 'energy_tot' prediction: ['Level 1 qualifications', 'Jewish', 'Mixed', 'Aged 0-15', 'Limited activities', 'Christian', 'Born in UK', 'Muslim', 'Sikh']\n"
     ]
    }
   ],
   "source": [
    "for y in target_cols:\n",
    "    print(f'features for \\'{y}\\' prediction:', selected_cols_best[y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_formula(cols):\n",
    "    cols = list(map(lambda s: 'Q(\"{}\")'.format(s), cols))\n",
    "    return ' + '.join(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear model for energy_fat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:             energy_fat   R-squared:                       0.369\n",
      "Model:                            OLS   Adj. R-squared:                  0.356\n",
      "Method:                 Least Squares   F-statistic:                     27.63\n",
      "Date:                Fri, 11 Dec 2020   Prob (F-statistic):           1.50e-41\n",
      "Time:                        01:33:49   Log-Likelihood:                -1160.8\n",
      "No. Observations:                 483   AIC:                             2344.\n",
      "Df Residuals:                     472   BIC:                             2390.\n",
      "Df Model:                          10                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===============================================================================================\n",
      "                                  coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Intercept                      67.4103      3.553     18.975      0.000      60.430      74.391\n",
      "Q(\"Level 1 qualifications\")    24.0243      5.969      4.025      0.000      12.294      35.754\n",
      "Q(\"Jewish\")                   -15.9587      5.044     -3.164      0.002     -25.869      -6.048\n",
      "Q(\"Mixed\")                    -14.0683     16.378     -0.859      0.391     -46.251      18.114\n",
      "Q(\"Aged 0-15\")                 20.9680      5.106      4.107      0.000      10.935      31.001\n",
      "Q(\"Christian\")                  6.5188      3.755      1.736      0.083      -0.860      13.897\n",
      "Q(\"Black or Black British\")   -12.3686      3.277     -3.775      0.000     -18.807      -5.930\n",
      "Q(\"Muslim\")                    12.5124      3.349      3.736      0.000       5.932      19.093\n",
      "Q(\"No religion\")               17.0790      4.681      3.648      0.000       7.880      26.278\n",
      "Q(\"Asian or Asian British\")     8.4919      3.127      2.716      0.007       2.348      14.636\n",
      "Q(\"IOD AVG\")                    0.1025      0.298      0.344      0.731      -0.483       0.688\n",
      "==============================================================================\n",
      "Omnibus:                      319.961   Durbin-Watson:                   1.684\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             8305.069\n",
      "Skew:                           2.431   Prob(JB):                         0.00\n",
      "Kurtosis:                      22.724   Cond. No.                         162.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "target = 'energy_fat'\n",
    "model = smf.ols(formula='{} ~ {}'.format(target, to_formula(selected_cols_best[target])),\n",
    "              data=df_ward_atlas)\n",
    "res = model.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistically significant features (with p-value < 0.05): Black or Black British, Asian or Asian British (etnicities), Jewish, Muslim, No religion (religions), Level 1 qualifications, Aged 0-15. More precisely, we see that amount of consumed energy from fats depends on:\n",
    "1) Etnicity: less for Black or Black British, more for Asian or Asian British. <br>\n",
    "2) Religion: less for Jewish, more for Muslim and people with no religion. <br>\n",
    "3) Qualification: more for people with the lowest qualification. <br>\n",
    "4) Age: more for children (age 0-15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear model for energy_saturate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:        energy_saturate   R-squared:                       0.379\n",
      "Model:                            OLS   Adj. R-squared:                  0.364\n",
      "Method:                 Least Squares   F-statistic:                     26.13\n",
      "Date:                Fri, 11 Dec 2020   Prob (F-statistic):           2.36e-42\n",
      "Time:                        01:43:30   Log-Likelihood:                -785.08\n",
      "No. Observations:                 483   AIC:                             1594.\n",
      "Df Residuals:                     471   BIC:                             1644.\n",
      "Df Model:                          11                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "======================================================================================================\n",
      "                                         coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------------------\n",
      "Intercept                             19.3212      1.586     12.183      0.000      16.205      22.438\n",
      "Q(\"Asian or Asian British\")           11.4181      1.327      8.604      0.000       8.811      14.026\n",
      "Q(\"Born in UK\")                       -1.3437      1.309     -1.027      0.305      -3.916       1.228\n",
      "Q(\"Muslim\")                            8.5449      1.264      6.760      0.000       6.061      11.029\n",
      "Q(\"White\")                            11.7840      1.532      7.690      0.000       8.773      14.795\n",
      "Q(\"Jewish\")                           -9.4583      1.442     -6.561      0.000     -12.291      -6.625\n",
      "Q(\"Aged 0-15\")                         6.8194      2.562      2.661      0.008       1.784      11.855\n",
      "Q(\"Mixed\")                            29.5564      8.063      3.666      0.000      13.712      45.401\n",
      "Q(\"Apprenticeship qualifications\")    -9.7483     16.780     -0.581      0.562     -42.722      23.225\n",
      "Q(\"Level 1 qualifications\")           12.5405      4.747      2.642      0.009       3.213      21.868\n",
      "Q(\"Level 3 qualifications\")           -6.7662      2.983     -2.269      0.024     -12.627      -0.905\n",
      "Q(\"Other religions\")                  23.5763     11.010      2.141      0.033       1.942      45.211\n",
      "==============================================================================\n",
      "Omnibus:                      349.324   Durbin-Watson:                   1.844\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            11454.779\n",
      "Skew:                           2.686   Prob(JB):                         0.00\n",
      "Kurtosis:                      26.245   Cond. No.                         420.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "target = 'energy_saturate'\n",
    "model = smf.ols(formula='{} ~ {}'.format(target, to_formula(selected_cols_best[target])),\n",
    "              data=df_ward_atlas)\n",
    "res = model.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear model for energy_sugar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:           energy_sugar   R-squared:                       0.602\n",
      "Model:                            OLS   Adj. R-squared:                  0.591\n",
      "Method:                 Least Squares   F-statistic:                     54.56\n",
      "Date:                Fri, 11 Dec 2020   Prob (F-statistic):           3.82e-85\n",
      "Time:                        01:45:33   Log-Likelihood:                -1112.2\n",
      "No. Observations:                 483   AIC:                             2252.\n",
      "Df Residuals:                     469   BIC:                             2311.\n",
      "Df Model:                          13                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "======================================================================================================\n",
      "                                         coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------------------\n",
      "Intercept                             19.5480      6.797      2.876      0.004       6.191      32.905\n",
      "Q(\"Level 1 qualifications\")           84.8109     10.708      7.921      0.000      63.770     105.852\n",
      "Q(\"Black or Black British\")           -0.4996      7.499     -0.067      0.947     -15.236      14.237\n",
      "Q(\"Bad Health\")                        0.3381      0.236      1.434      0.152      -0.125       0.801\n",
      "Q(\"White\")                             5.4827      7.438      0.737      0.461      -9.134      20.099\n",
      "Q(\"Hindu\")                            -5.5022      3.875     -1.420      0.156     -13.117       2.112\n",
      "Q(\"Mixed\")                           -15.2619     18.441     -0.828      0.408     -51.500      20.976\n",
      "Q(\"Asian or Asian British\")           17.8987      6.871      2.605      0.009       4.397      31.400\n",
      "Q(\"No religion\")                      13.5548      3.947      3.435      0.001       5.800      21.310\n",
      "Q(\"Aged 0-15\")                         3.6617      5.021      0.729      0.466      -6.205      13.528\n",
      "Q(\"Christian\")                         7.9276      2.454      3.231      0.001       3.106      12.749\n",
      "Q(\"Muslim\")                            5.3225      3.693      1.441      0.150      -1.935      12.580\n",
      "Q(\"IOD AVG\")                          -1.0930      0.351     -3.114      0.002      -1.783      -0.403\n",
      "Q(\"Apprenticeship qualifications\")   -68.5670     37.584     -1.824      0.069    -142.422       5.288\n",
      "==============================================================================\n",
      "Omnibus:                      396.803   Durbin-Watson:                   1.689\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            20469.464\n",
      "Skew:                           3.100   Prob(JB):                         0.00\n",
      "Kurtosis:                      34.284   Cond. No.                         480.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "target = 'energy_sugar'\n",
    "model = smf.ols(formula='{} ~ {}'.format(target, to_formula(selected_cols_best[target])),\n",
    "              data=df_ward_atlas)\n",
    "res = model.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear model for energy_protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:         energy_protein   R-squared:                       0.639\n",
      "Model:                            OLS   Adj. R-squared:                  0.628\n",
      "Method:                 Least Squares   F-statistic:                     55.17\n",
      "Date:                Fri, 11 Dec 2020   Prob (F-statistic):           3.27e-93\n",
      "Time:                        01:47:22   Log-Likelihood:                -384.19\n",
      "No. Observations:                 483   AIC:                             800.4\n",
      "Df Residuals:                     467   BIC:                             867.3\n",
      "Df Model:                          15                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "======================================================================================================\n",
      "                                         coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------------------\n",
      "Intercept                             21.8057      1.239     17.595      0.000      19.370      24.241\n",
      "Q(\"Asian or Asian British\")           -7.4556      0.655    -11.374      0.000      -8.744      -6.168\n",
      "Q(\"Jewish\")                           -9.4217      1.059     -8.899      0.000     -11.502      -7.341\n",
      "Q(\"Level 1 qualifications\")          -11.1822      3.584     -3.120      0.002     -18.225      -4.140\n",
      "Q(\"Sikh\")                              4.3579      0.816      5.339      0.000       2.754       5.962\n",
      "Q(\"Muslim\")                            0.7174      0.692      1.037      0.300      -0.642       2.077\n",
      "Q(\"No religion\")                      -6.4904      1.274     -5.096      0.000      -8.993      -3.988\n",
      "Q(\"Level 2 qualifications\")            0.7922      3.438      0.230      0.818      -5.964       7.548\n",
      "Q(\"Christian\")                        -2.3165      0.781     -2.966      0.003      -3.851      -0.782\n",
      "Q(\"IOD AVG\")                          -0.1146      0.067     -1.710      0.088      -0.246       0.017\n",
      "Q(\"Household Income Median\")          -0.2228      0.071     -3.131      0.002      -0.363      -0.083\n",
      "Q(\"Apprenticeship qualifications\")     6.2486      8.171      0.765      0.445      -9.807      22.305\n",
      "Q(\"Buddhist\")                         22.4165      6.419      3.492      0.001       9.803      35.030\n",
      "Q(\"Level >=4 qualifications\")          0.8106      0.941      0.861      0.390      -1.039       2.661\n",
      "Q(\"Born in UK\")                        2.4869      0.747      3.330      0.001       1.019       3.955\n",
      "Q(\"Aged 16-64\")                        2.8856      1.084      2.662      0.008       0.755       5.016\n",
      "==============================================================================\n",
      "Omnibus:                        9.666   Durbin-Watson:                   1.619\n",
      "Prob(Omnibus):                  0.008   Jarque-Bera (JB):               12.338\n",
      "Skew:                          -0.204   Prob(JB):                      0.00209\n",
      "Kurtosis:                       3.668   Cond. No.                         520.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "target = 'energy_protein'\n",
    "model = smf.ols(formula='{} ~ {}'.format(target, to_formula(selected_cols_best[target])),\n",
    "              data=df_ward_atlas)\n",
    "res = model.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear model for energy_carb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            energy_carb   R-squared:                       0.775\n",
      "Model:                            OLS   Adj. R-squared:                  0.771\n",
      "Method:                 Least Squares   F-statistic:                     181.4\n",
      "Date:                Fri, 11 Dec 2020   Prob (F-statistic):          3.14e-147\n",
      "Time:                        01:44:00   Log-Likelihood:                -1151.7\n",
      "No. Observations:                 483   AIC:                             2323.\n",
      "Df Residuals:                     473   BIC:                             2365.\n",
      "Df Model:                           9                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=================================================================================================\n",
      "                                    coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Intercept                        75.0937      3.271     22.957      0.000      68.666      81.521\n",
      "Q(\"Level 1 qualifications\")      30.7539     16.339      1.882      0.060      -1.352      62.860\n",
      "Q(\"Mixed\")                      -89.8382      9.485     -9.472      0.000    -108.476     -71.200\n",
      "Q(\"Jewish\")                     -15.4931      2.960     -5.234      0.000     -21.310      -9.676\n",
      "Q(\"White\")                      -14.5215      1.826     -7.953      0.000     -18.110     -10.933\n",
      "Q(\"Born in UK\")                  19.6703      2.786      7.061      0.000      14.197      25.144\n",
      "Q(\"Other religions\")             -8.7660     21.287     -0.412      0.681     -50.595      33.063\n",
      "Q(\"Limited activities\")          -0.4544      0.222     -2.047      0.041      -0.891      -0.018\n",
      "Q(\"Level >=4 qualifications\")   -13.3712      4.657     -2.871      0.004     -22.523      -4.220\n",
      "Q(\"No qualifications\")            4.2454      7.360      0.577      0.564     -10.216      18.707\n",
      "==============================================================================\n",
      "Omnibus:                       27.240   Durbin-Watson:                   1.588\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               41.883\n",
      "Skew:                           0.422   Prob(JB):                     8.04e-10\n",
      "Kurtosis:                       4.170   Cond. No.                         252.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "target = 'energy_carb'\n",
    "model = smf.ols(formula='{} ~ {}'.format(target, to_formula(selected_cols_best[target])),\n",
    "              data=df_ward_atlas)\n",
    "res = model.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistically significant features (with p-value < 0.05): White, Mixed (etnicities), Jewish (religion), Born in UK, Limited activities, Level >=4 qualifications (i. e. maximum level of qualification). More precisely, we see that amount of energy consumed from carbohidrates depends on:\n",
    "1) Etnicity: less for white and (especially) mixed people than in average\n",
    "2) Religion: less for Jewish people\n",
    "3) Place of birth: more for people born in UK than for those who were born abroad\n",
    "4) Health state: less for people whose Day-to-day activities are limited a lot\n",
    "5) Qualification: less for highly qualified people, more for low qualified. Here results for 'No qualifications' may be skewed, because this category probably includes all children independently of their mental abilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear model for energy_fibre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:           energy_fibre   R-squared:                       0.605\n",
      "Model:                            OLS   Adj. R-squared:                  0.596\n",
      "Method:                 Least Squares   F-statistic:                     72.19\n",
      "Date:                Fri, 11 Dec 2020   Prob (F-statistic):           1.40e-88\n",
      "Time:                        01:48:23   Log-Likelihood:                 485.00\n",
      "No. Observations:                 483   AIC:                            -948.0\n",
      "Df Residuals:                     472   BIC:                            -902.0\n",
      "Df Model:                          10                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "================================================================================================\n",
      "                                   coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------------\n",
      "Intercept                        3.3963      0.130     26.211      0.000       3.142       3.651\n",
      "Q(\"IOD AVG\")                    -0.0385      0.011     -3.550      0.000      -0.060      -0.017\n",
      "Q(\"No religion\")                 0.0026      0.134      0.019      0.985      -0.260       0.265\n",
      "Q(\"Level 1 qualifications\")     -1.4472      0.347     -4.173      0.000      -2.129      -0.766\n",
      "Q(\"Sikh\")                        0.8129      0.123      6.583      0.000       0.570       1.056\n",
      "Q(\"White\")                       0.3936      0.073      5.417      0.000       0.251       0.536\n",
      "Q(\"Aged 16-64\")                 -0.4994      0.153     -3.255      0.001      -0.801      -0.198\n",
      "Q(\"Household Income Median\")    -0.0422      0.012     -3.630      0.000      -0.065      -0.019\n",
      "Q(\"Jewish\")                     -0.0515      0.105     -0.489      0.625      -0.258       0.155\n",
      "Q(\"No qualifications\")          -0.5426      0.220     -2.462      0.014      -0.976      -0.110\n",
      "Q(\"Black or Black British\")     -0.0833      0.089     -0.937      0.349      -0.258       0.091\n",
      "==============================================================================\n",
      "Omnibus:                       94.543   Durbin-Watson:                   1.863\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              352.100\n",
      "Skew:                           0.843   Prob(JB):                     3.49e-77\n",
      "Kurtosis:                       6.828   Cond. No.                         131.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "target = 'energy_fibre'\n",
    "model = smf.ols(formula='{} ~ {}'.format(target, to_formula(selected_cols_best[target])),\n",
    "              data=df_ward_atlas)\n",
    "res = model.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear model for energy_alcohol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:         energy_alcohol   R-squared:                       0.413\n",
      "Model:                            OLS   Adj. R-squared:                  0.403\n",
      "Method:                 Least Squares   F-statistic:                     41.65\n",
      "Date:                Fri, 11 Dec 2020   Prob (F-statistic):           2.60e-50\n",
      "Time:                        01:49:36   Log-Likelihood:                -63.078\n",
      "No. Observations:                 483   AIC:                             144.2\n",
      "Df Residuals:                     474   BIC:                             181.8\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "======================================================================================================\n",
      "                                         coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------------------\n",
      "Intercept                              1.5975      0.155     10.306      0.000       1.293       1.902\n",
      "Q(\"Level >=4 qualifications\")          1.3676      0.262      5.226      0.000       0.853       1.882\n",
      "Q(\"No religion\")                      -0.5177      0.360     -1.440      0.151      -1.224       0.189\n",
      "Q(\"Jewish\")                           -1.2948      0.322     -4.016      0.000      -1.928      -0.661\n",
      "Q(\"Muslim\")                           -1.9564      0.316     -6.196      0.000      -2.577      -1.336\n",
      "Q(\"Apprenticeship qualifications\")    -3.0683      3.260     -0.941      0.347      -9.475       3.338\n",
      "Q(\"Other religions\")                  -6.9563      2.324     -2.993      0.003     -11.524      -2.389\n",
      "Q(\"IOD AVG\")                           0.0541      0.031      1.753      0.080      -0.007       0.115\n",
      "Q(\"Bad Health\")                        0.0003      0.023      0.012      0.991      -0.044       0.045\n",
      "==============================================================================\n",
      "Omnibus:                       38.709   Durbin-Watson:                   1.982\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               54.739\n",
      "Skew:                           0.598   Prob(JB):                     1.30e-12\n",
      "Kurtosis:                       4.135   Cond. No.                         343.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "target = 'energy_alcohol'\n",
    "model = smf.ols(formula='{} ~ {}'.format(target, to_formula(selected_cols_best[target])),\n",
    "              data=df_ward_atlas)\n",
    "res = model.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear model for energy_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:             energy_tot   R-squared:                       0.661\n",
      "Model:                            OLS   Adj. R-squared:                  0.655\n",
      "Method:                 Least Squares   F-statistic:                     102.5\n",
      "Date:                Fri, 11 Dec 2020   Prob (F-statistic):          3.32e-105\n",
      "Time:                        01:51:37   Log-Likelihood:                -1405.3\n",
      "No. Observations:                 483   AIC:                             2831.\n",
      "Df Residuals:                     473   BIC:                             2872.\n",
      "Df Model:                           9                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===============================================================================================\n",
      "                                  coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Intercept                     164.6510      2.750     59.866      0.000     159.247     170.055\n",
      "Q(\"Level 1 qualifications\")   101.5394     11.116      9.135      0.000      79.697     123.382\n",
      "Q(\"Jewish\")                   -55.5082      5.720     -9.704      0.000     -66.748     -44.268\n",
      "Q(\"Mixed\")                   -113.5997     16.952     -6.701      0.000    -146.911     -80.289\n",
      "Q(\"Aged 0-15\")                 22.0867      8.383      2.635      0.009       5.615      38.559\n",
      "Q(\"Limited activities\")        -0.4854      0.274     -1.770      0.077      -1.024       0.053\n",
      "Q(\"Christian\")                 -7.9466      3.788     -2.098      0.036     -15.390      -0.503\n",
      "Q(\"Born in UK\")                13.9908      3.062      4.568      0.000       7.973      20.009\n",
      "Q(\"Muslim\")                    14.4069      4.614      3.122      0.002       5.340      23.474\n",
      "Q(\"Sikh\")                       4.6586      6.493      0.718      0.473      -8.099      17.417\n",
      "==============================================================================\n",
      "Omnibus:                      205.635   Durbin-Watson:                   1.664\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2606.348\n",
      "Skew:                           1.485   Prob(JB):                         0.00\n",
      "Kurtosis:                      13.986   Cond. No.                         116.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "target = 'energy_tot'\n",
    "model = smf.ols(formula='{} ~ {}'.format(target, to_formula(selected_cols_best[target])),\n",
    "              data=df_ward_atlas)\n",
    "res = model.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting a fraction of each nutrient energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_energy_fat: best r2 score = 0.47521, num features = 9\n",
      "f_energy_saturate: best r2 score = 0.43184, num features = 10\n",
      "f_energy_sugar: best r2 score = 0.43231, num features = 8\n",
      "f_energy_protein: best r2 score = 0.70012, num features = 8\n",
      "f_energy_carb: best r2 score = 0.72733, num features = 8\n",
      "f_energy_fibre: best r2 score = 0.76821, num features = 15\n",
      "f_energy_alcohol: best r2 score = 0.45613, num features = 14\n"
     ]
    }
   ],
   "source": [
    "target_cols = ['f_energy_fat', 'f_energy_saturate', \n",
    "                 'f_energy_sugar', 'f_energy_protein', 'f_energy_carb', 'f_energy_fibre', 'f_energy_alcohol']\n",
    "global_test_best = {}\n",
    "selected_cols_best = {}\n",
    "energy_corr = {}\n",
    "for y in target_cols:\n",
    "    energy_corr[y] = df_ward_atlas[all_features].corrwith(df_ward_atlas[y], method='pearson')\n",
    "    energy_corr[y] = energy_corr[y].sort_values(ascending=False, key=np.abs)\n",
    "    #get rid of linearly dependent cols\n",
    "    for cols in [age_cols, ethnicities_cols, religion_cols, born_cols, qualilication_cols]:\n",
    "        energy_corr[y].drop(energy_corr[y][cols].index[-1], axis=0, inplace=True)\n",
    "    global_test_best[y], selected_cols_best[y]  = forward_feature_selection_brute_force(energy_corr[y], y)\n",
    "    print('{}: best r2 score = {:.5f}, num features = {}'.format(y, global_test_best[y], len(selected_cols_best[y])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features for 'f_energy_fat' prediction: ['Level 2 qualifications', 'Black or Black British', 'Mixed', 'Apprenticeship qualifications', 'Born in UK', 'Aged 0-15', 'Hindu', 'No religion', 'No qualifications']\n",
      "features for 'f_energy_saturate' prediction: ['Level 2 qualifications', 'No religion', 'Black or Black British', 'Mixed', 'Muslim', 'Aged 16-64', 'Other religions', 'Level 1 qualifications', 'No qualifications', 'Level 3 qualifications']\n",
      "features for 'f_energy_sugar' prediction: ['Level 1 qualifications', 'Black or Black British', 'Asian or Asian British', 'IOD AVG', 'No religion', 'Apprenticeship qualifications', 'Jewish', 'Christian']\n",
      "features for 'f_energy_protein' prediction: ['Level >=4 qualifications', 'Asian or Asian British', 'Level 1 qualifications', 'Mixed', 'Aged 0-15', 'Sikh', 'No religion', 'Christian']\n",
      "features for 'f_energy_carb' prediction: ['Level 1 qualifications', 'Hindu', 'Level 2 qualifications', 'White', 'Mixed', 'Born in UK', 'Sikh', 'Black or Black British']\n",
      "features for 'f_energy_fibre' prediction: ['Level >=4 qualifications', 'Jewish', 'White', 'Level 3 qualifications', 'IOD AVG', 'Level 2 qualifications', 'Sikh', 'Asian or Asian British', 'Level 1 qualifications', 'Buddhist', 'Hindu', 'No qualifications', 'Limited activities', 'Household Income Median', 'Aged 16-64']\n",
      "features for 'f_energy_alcohol' prediction: ['Level >=4 qualifications', 'No religion', 'Christian', 'Level 2 qualifications', 'Muslim', 'IOD AVG', 'Level 1 qualifications', 'Jewish', 'Black or Black British', 'White', 'Asian or Asian British', 'Limited activities', 'Mixed', 'Other religions']\n"
     ]
    }
   ],
   "source": [
    "for y in target_cols:\n",
    "    print(f'features for \\'{y}\\' prediction:', selected_cols_best[y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols_best = {}\n",
    "selected_cols_best['f_energy_fat'] = ['Level 2 qualifications', 'Black or Black British', 'Mixed', \n",
    "                                      'Apprenticeship qualifications', 'Born in UK', 'Aged 0-15', 'Hindu', \n",
    "                                      'No religion', 'No qualifications']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:           f_energy_fat   R-squared:                       0.494\n",
      "Model:                            OLS   Adj. R-squared:                  0.484\n",
      "Method:                 Least Squares   F-statistic:                     51.27\n",
      "Date:                Sat, 12 Dec 2020   Prob (F-statistic):           1.83e-64\n",
      "Time:                        16:55:49   Log-Likelihood:                 1655.6\n",
      "No. Observations:                 483   AIC:                            -3291.\n",
      "Df Residuals:                     473   BIC:                            -3249.\n",
      "Df Model:                           9                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "======================================================================================================\n",
      "                                         coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------------------\n",
      "Intercept                              0.4785      0.004    111.437      0.000       0.470       0.487\n",
      "Q(\"Level 2 qualifications\")           -0.0804      0.039     -2.057      0.040      -0.157      -0.004\n",
      "Q(\"Black or Black British\")           -0.0556      0.009     -6.511      0.000      -0.072      -0.039\n",
      "Q(\"Mixed\")                             0.0852      0.046      1.866      0.063      -0.005       0.175\n",
      "Q(\"Apprenticeship qualifications\")    -0.1531      0.102     -1.498      0.135      -0.354       0.048\n",
      "Q(\"Born in UK\")                       -0.0335      0.009     -3.771      0.000      -0.051      -0.016\n",
      "Q(\"Aged 0-15\")                         0.0482      0.015      3.162      0.002       0.018       0.078\n",
      "Q(\"Hindu\")                            -0.0268      0.007     -3.586      0.000      -0.042      -0.012\n",
      "Q(\"No religion\")                       0.0182      0.012      1.547      0.122      -0.005       0.041\n",
      "Q(\"No qualifications\")                -0.0139      0.010     -1.323      0.186      -0.035       0.007\n",
      "==============================================================================\n",
      "Omnibus:                       56.412   Durbin-Watson:                   1.668\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              307.972\n",
      "Skew:                           0.298   Prob(JB):                     1.33e-67\n",
      "Kurtosis:                       6.866   Cond. No.                         361.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "target = 'f_energy_fat'\n",
    "model = smf.ols(formula='{} ~ {}'.format(target, to_formula(selected_cols_best[target])),\n",
    "              data=df_ward_atlas)\n",
    "res = model.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network for predicting all at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.losses import KLDivergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = ['f_energy_fat', 'f_energy_protein', 'f_energy_carb', 'f_energy_fibre', 'f_energy_alcohol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "E05000026    1.0\n",
       "E05000027    1.0\n",
       "E05000028    1.0\n",
       "E05000029    1.0\n",
       "E05000030    1.0\n",
       "            ... \n",
       "E05000645    1.0\n",
       "E05000646    1.0\n",
       "E05000647    1.0\n",
       "E05000648    1.0\n",
       "E05000649    1.0\n",
       "Length: 483, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ward_atlas[target_cols].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any(np.abs(df_ward_atlas[target_cols].sum(axis=1) - 1 > 0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(30,)))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.compile(optimizer='sgd', \n",
    "              loss=KLDivergence(), \n",
    "              metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = df_ward_atlas[all_features]\n",
    "y_data = df_ward_atlas[target_cols]\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 386 samples, validate on 97 samples\n",
      "Epoch 1/300\n",
      "386/386 [==============================] - 0s 223us/sample - loss: 0.6217 - mean_absolute_error: 0.1945 - val_loss: 0.4717 - val_mean_absolute_error: 0.1681\n",
      "Epoch 2/300\n",
      "386/386 [==============================] - 0s 38us/sample - loss: 0.5361 - mean_absolute_error: 0.1789 - val_loss: 0.4070 - val_mean_absolute_error: 0.1544\n",
      "Epoch 3/300\n",
      "386/386 [==============================] - 0s 47us/sample - loss: 0.4643 - mean_absolute_error: 0.1648 - val_loss: 0.3514 - val_mean_absolute_error: 0.1417\n",
      "Epoch 4/300\n",
      "386/386 [==============================] - 0s 54us/sample - loss: 0.4010 - mean_absolute_error: 0.1510 - val_loss: 0.3049 - val_mean_absolute_error: 0.1301\n",
      "Epoch 5/300\n",
      "386/386 [==============================] - 0s 54us/sample - loss: 0.3477 - mean_absolute_error: 0.1383 - val_loss: 0.2665 - val_mean_absolute_error: 0.1194\n",
      "Epoch 6/300\n",
      "386/386 [==============================] - 0s 53us/sample - loss: 0.3057 - mean_absolute_error: 0.1273 - val_loss: 0.2344 - val_mean_absolute_error: 0.1099\n",
      "Epoch 7/300\n",
      "386/386 [==============================] - 0s 52us/sample - loss: 0.2700 - mean_absolute_error: 0.1173 - val_loss: 0.2071 - val_mean_absolute_error: 0.1011\n",
      "Epoch 8/300\n",
      "386/386 [==============================] - 0s 49us/sample - loss: 0.2402 - mean_absolute_error: 0.1083 - val_loss: 0.1838 - val_mean_absolute_error: 0.0934\n",
      "Epoch 9/300\n",
      "386/386 [==============================] - 0s 56us/sample - loss: 0.2139 - mean_absolute_error: 0.1000 - val_loss: 0.1640 - val_mean_absolute_error: 0.0865\n",
      "Epoch 10/300\n",
      "386/386 [==============================] - 0s 52us/sample - loss: 0.1916 - mean_absolute_error: 0.0926 - val_loss: 0.1472 - val_mean_absolute_error: 0.0803\n",
      "Epoch 11/300\n",
      "386/386 [==============================] - 0s 54us/sample - loss: 0.1729 - mean_absolute_error: 0.0861 - val_loss: 0.1328 - val_mean_absolute_error: 0.0747\n",
      "Epoch 12/300\n",
      "386/386 [==============================] - 0s 49us/sample - loss: 0.1567 - mean_absolute_error: 0.0801 - val_loss: 0.1202 - val_mean_absolute_error: 0.0697\n",
      "Epoch 13/300\n",
      "386/386 [==============================] - 0s 52us/sample - loss: 0.1420 - mean_absolute_error: 0.0746 - val_loss: 0.1094 - val_mean_absolute_error: 0.0652\n",
      "Epoch 14/300\n",
      "386/386 [==============================] - 0s 52us/sample - loss: 0.1298 - mean_absolute_error: 0.0698 - val_loss: 0.1002 - val_mean_absolute_error: 0.0613\n",
      "Epoch 15/300\n",
      "386/386 [==============================] - 0s 52us/sample - loss: 0.1192 - mean_absolute_error: 0.0655 - val_loss: 0.0921 - val_mean_absolute_error: 0.0577\n",
      "Epoch 16/300\n",
      "386/386 [==============================] - 0s 52us/sample - loss: 0.1100 - mean_absolute_error: 0.0616 - val_loss: 0.0849 - val_mean_absolute_error: 0.0544\n",
      "Epoch 17/300\n",
      "386/386 [==============================] - 0s 49us/sample - loss: 0.1019 - mean_absolute_error: 0.0581 - val_loss: 0.0787 - val_mean_absolute_error: 0.0514\n",
      "Epoch 18/300\n",
      "386/386 [==============================] - 0s 59us/sample - loss: 0.0949 - mean_absolute_error: 0.0549 - val_loss: 0.0731 - val_mean_absolute_error: 0.0487\n",
      "Epoch 19/300\n",
      "386/386 [==============================] - 0s 52us/sample - loss: 0.0884 - mean_absolute_error: 0.0520 - val_loss: 0.0681 - val_mean_absolute_error: 0.0463\n",
      "Epoch 20/300\n",
      "386/386 [==============================] - 0s 52us/sample - loss: 0.0828 - mean_absolute_error: 0.0494 - val_loss: 0.0636 - val_mean_absolute_error: 0.0440\n",
      "Epoch 21/300\n",
      "386/386 [==============================] - 0s 62us/sample - loss: 0.0775 - mean_absolute_error: 0.0469 - val_loss: 0.0597 - val_mean_absolute_error: 0.0420\n",
      "Epoch 22/300\n",
      "386/386 [==============================] - 0s 72us/sample - loss: 0.0729 - mean_absolute_error: 0.0447 - val_loss: 0.0561 - val_mean_absolute_error: 0.0401\n",
      "Epoch 23/300\n",
      "386/386 [==============================] - 0s 67us/sample - loss: 0.0687 - mean_absolute_error: 0.0426 - val_loss: 0.0529 - val_mean_absolute_error: 0.0384\n",
      "Epoch 24/300\n",
      "386/386 [==============================] - 0s 57us/sample - loss: 0.0650 - mean_absolute_error: 0.0408 - val_loss: 0.0500 - val_mean_absolute_error: 0.0369\n",
      "Epoch 25/300\n",
      "386/386 [==============================] - 0s 65us/sample - loss: 0.0616 - mean_absolute_error: 0.0391 - val_loss: 0.0473 - val_mean_absolute_error: 0.0354\n",
      "Epoch 26/300\n",
      "386/386 [==============================] - 0s 72us/sample - loss: 0.0585 - mean_absolute_error: 0.0375 - val_loss: 0.0449 - val_mean_absolute_error: 0.0340\n",
      "Epoch 27/300\n",
      "386/386 [==============================] - 0s 83us/sample - loss: 0.0557 - mean_absolute_error: 0.0361 - val_loss: 0.0427 - val_mean_absolute_error: 0.0328\n",
      "Epoch 28/300\n",
      "386/386 [==============================] - 0s 83us/sample - loss: 0.0531 - mean_absolute_error: 0.0348 - val_loss: 0.0407 - val_mean_absolute_error: 0.0316\n",
      "Epoch 29/300\n",
      "386/386 [==============================] - 0s 85us/sample - loss: 0.0507 - mean_absolute_error: 0.0336 - val_loss: 0.0388 - val_mean_absolute_error: 0.0306\n",
      "Epoch 30/300\n",
      "386/386 [==============================] - 0s 85us/sample - loss: 0.0484 - mean_absolute_error: 0.0324 - val_loss: 0.0371 - val_mean_absolute_error: 0.0296\n",
      "Epoch 31/300\n",
      "386/386 [==============================] - 0s 88us/sample - loss: 0.0463 - mean_absolute_error: 0.0314 - val_loss: 0.0355 - val_mean_absolute_error: 0.0288\n",
      "Epoch 32/300\n",
      "386/386 [==============================] - 0s 72us/sample - loss: 0.0444 - mean_absolute_error: 0.0304 - val_loss: 0.0341 - val_mean_absolute_error: 0.0279\n",
      "Epoch 33/300\n",
      "386/386 [==============================] - 0s 62us/sample - loss: 0.0426 - mean_absolute_error: 0.0295 - val_loss: 0.0326 - val_mean_absolute_error: 0.0271\n",
      "Epoch 34/300\n",
      "386/386 [==============================] - 0s 80us/sample - loss: 0.0408 - mean_absolute_error: 0.0286 - val_loss: 0.0314 - val_mean_absolute_error: 0.0264\n",
      "Epoch 35/300\n",
      "386/386 [==============================] - 0s 75us/sample - loss: 0.0392 - mean_absolute_error: 0.0279 - val_loss: 0.0302 - val_mean_absolute_error: 0.0258\n",
      "Epoch 36/300\n",
      "386/386 [==============================] - 0s 78us/sample - loss: 0.0377 - mean_absolute_error: 0.0271 - val_loss: 0.0290 - val_mean_absolute_error: 0.0251\n",
      "Epoch 37/300\n",
      "386/386 [==============================] - 0s 70us/sample - loss: 0.0363 - mean_absolute_error: 0.0265 - val_loss: 0.0280 - val_mean_absolute_error: 0.0244\n",
      "Epoch 38/300\n",
      "386/386 [==============================] - 0s 59us/sample - loss: 0.0351 - mean_absolute_error: 0.0258 - val_loss: 0.0270 - val_mean_absolute_error: 0.0239\n",
      "Epoch 39/300\n",
      "386/386 [==============================] - 0s 46us/sample - loss: 0.0339 - mean_absolute_error: 0.0252 - val_loss: 0.0261 - val_mean_absolute_error: 0.0234\n",
      "Epoch 40/300\n",
      "386/386 [==============================] - 0s 67us/sample - loss: 0.0328 - mean_absolute_error: 0.0247 - val_loss: 0.0253 - val_mean_absolute_error: 0.0229\n",
      "Epoch 41/300\n",
      "386/386 [==============================] - 0s 72us/sample - loss: 0.0317 - mean_absolute_error: 0.0242 - val_loss: 0.0245 - val_mean_absolute_error: 0.0225\n",
      "Epoch 42/300\n",
      "386/386 [==============================] - 0s 75us/sample - loss: 0.0307 - mean_absolute_error: 0.0237 - val_loss: 0.0237 - val_mean_absolute_error: 0.0220\n",
      "Epoch 43/300\n",
      "386/386 [==============================] - 0s 59us/sample - loss: 0.0297 - mean_absolute_error: 0.0232 - val_loss: 0.0230 - val_mean_absolute_error: 0.0216\n",
      "Epoch 44/300\n",
      "386/386 [==============================] - 0s 70us/sample - loss: 0.0287 - mean_absolute_error: 0.0228 - val_loss: 0.0223 - val_mean_absolute_error: 0.0213\n",
      "Epoch 45/300\n",
      "386/386 [==============================] - 0s 62us/sample - loss: 0.0278 - mean_absolute_error: 0.0224 - val_loss: 0.0216 - val_mean_absolute_error: 0.0209\n",
      "Epoch 46/300\n",
      "386/386 [==============================] - 0s 72us/sample - loss: 0.0270 - mean_absolute_error: 0.0220 - val_loss: 0.0210 - val_mean_absolute_error: 0.0205\n",
      "Epoch 47/300\n",
      "386/386 [==============================] - 0s 62us/sample - loss: 0.0262 - mean_absolute_error: 0.0216 - val_loss: 0.0204 - val_mean_absolute_error: 0.0202\n",
      "Epoch 48/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "386/386 [==============================] - 0s 72us/sample - loss: 0.0254 - mean_absolute_error: 0.0213 - val_loss: 0.0198 - val_mean_absolute_error: 0.0199\n",
      "Epoch 49/300\n",
      "386/386 [==============================] - 0s 67us/sample - loss: 0.0247 - mean_absolute_error: 0.0209 - val_loss: 0.0193 - val_mean_absolute_error: 0.0196\n",
      "Epoch 50/300\n",
      "386/386 [==============================] - 0s 57us/sample - loss: 0.0240 - mean_absolute_error: 0.0206 - val_loss: 0.0188 - val_mean_absolute_error: 0.0193\n",
      "Epoch 51/300\n",
      "386/386 [==============================] - 0s 62us/sample - loss: 0.0233 - mean_absolute_error: 0.0203 - val_loss: 0.0183 - val_mean_absolute_error: 0.0191\n",
      "Epoch 52/300\n",
      "386/386 [==============================] - 0s 54us/sample - loss: 0.0227 - mean_absolute_error: 0.0200 - val_loss: 0.0178 - val_mean_absolute_error: 0.0188\n",
      "Epoch 53/300\n",
      "386/386 [==============================] - 0s 49us/sample - loss: 0.0221 - mean_absolute_error: 0.0197 - val_loss: 0.0173 - val_mean_absolute_error: 0.0186\n",
      "Epoch 54/300\n",
      "386/386 [==============================] - 0s 54us/sample - loss: 0.0215 - mean_absolute_error: 0.0195 - val_loss: 0.0169 - val_mean_absolute_error: 0.0184\n",
      "Epoch 55/300\n",
      "386/386 [==============================] - 0s 52us/sample - loss: 0.0209 - mean_absolute_error: 0.0192 - val_loss: 0.0165 - val_mean_absolute_error: 0.0181\n",
      "Epoch 56/300\n",
      "386/386 [==============================] - 0s 65us/sample - loss: 0.0204 - mean_absolute_error: 0.0190 - val_loss: 0.0160 - val_mean_absolute_error: 0.0179\n",
      "Epoch 57/300\n",
      "386/386 [==============================] - 0s 57us/sample - loss: 0.0199 - mean_absolute_error: 0.0187 - val_loss: 0.0157 - val_mean_absolute_error: 0.0177\n",
      "Epoch 58/300\n",
      "386/386 [==============================] - 0s 67us/sample - loss: 0.0194 - mean_absolute_error: 0.0185 - val_loss: 0.0153 - val_mean_absolute_error: 0.0175\n",
      "Epoch 59/300\n",
      "386/386 [==============================] - 0s 67us/sample - loss: 0.0189 - mean_absolute_error: 0.0183 - val_loss: 0.0149 - val_mean_absolute_error: 0.0173\n",
      "Epoch 60/300\n",
      "386/386 [==============================] - 0s 72us/sample - loss: 0.0184 - mean_absolute_error: 0.0181 - val_loss: 0.0146 - val_mean_absolute_error: 0.0172\n",
      "Epoch 61/300\n",
      "386/386 [==============================] - 0s 60us/sample - loss: 0.0180 - mean_absolute_error: 0.0179 - val_loss: 0.0143 - val_mean_absolute_error: 0.0170\n",
      "Epoch 62/300\n",
      "386/386 [==============================] - 0s 54us/sample - loss: 0.0176 - mean_absolute_error: 0.0177 - val_loss: 0.0139 - val_mean_absolute_error: 0.0169\n",
      "Epoch 63/300\n",
      "386/386 [==============================] - 0s 54us/sample - loss: 0.0171 - mean_absolute_error: 0.0175 - val_loss: 0.0136 - val_mean_absolute_error: 0.0167\n",
      "Epoch 64/300\n",
      "386/386 [==============================] - 0s 47us/sample - loss: 0.0167 - mean_absolute_error: 0.0173 - val_loss: 0.0133 - val_mean_absolute_error: 0.0165\n",
      "Epoch 65/300\n",
      "386/386 [==============================] - 0s 52us/sample - loss: 0.0163 - mean_absolute_error: 0.0171 - val_loss: 0.0130 - val_mean_absolute_error: 0.0164\n",
      "Epoch 66/300\n",
      "386/386 [==============================] - 0s 57us/sample - loss: 0.0159 - mean_absolute_error: 0.0170 - val_loss: 0.0128 - val_mean_absolute_error: 0.0162\n",
      "Epoch 67/300\n",
      "386/386 [==============================] - 0s 49us/sample - loss: 0.0156 - mean_absolute_error: 0.0168 - val_loss: 0.0125 - val_mean_absolute_error: 0.0161\n",
      "Epoch 68/300\n",
      "386/386 [==============================] - 0s 54us/sample - loss: 0.0152 - mean_absolute_error: 0.0166 - val_loss: 0.0122 - val_mean_absolute_error: 0.0159\n",
      "Epoch 69/300\n",
      "386/386 [==============================] - 0s 49us/sample - loss: 0.0149 - mean_absolute_error: 0.0165 - val_loss: 0.0120 - val_mean_absolute_error: 0.0158\n",
      "Epoch 70/300\n",
      "386/386 [==============================] - 0s 47us/sample - loss: 0.0146 - mean_absolute_error: 0.0163 - val_loss: 0.0117 - val_mean_absolute_error: 0.0157\n",
      "Epoch 71/300\n",
      "386/386 [==============================] - 0s 47us/sample - loss: 0.0142 - mean_absolute_error: 0.0162 - val_loss: 0.0115 - val_mean_absolute_error: 0.0156\n",
      "Epoch 72/300\n",
      "386/386 [==============================] - 0s 44us/sample - loss: 0.0139 - mean_absolute_error: 0.0160 - val_loss: 0.0112 - val_mean_absolute_error: 0.0154\n",
      "Epoch 73/300\n",
      "386/386 [==============================] - 0s 44us/sample - loss: 0.0136 - mean_absolute_error: 0.0159 - val_loss: 0.0110 - val_mean_absolute_error: 0.0153\n",
      "Epoch 74/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0133 - mean_absolute_error: 0.0157 - val_loss: 0.0108 - val_mean_absolute_error: 0.0152\n",
      "Epoch 75/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0131 - mean_absolute_error: 0.0156 - val_loss: 0.0106 - val_mean_absolute_error: 0.0151\n",
      "Epoch 76/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0128 - mean_absolute_error: 0.0155 - val_loss: 0.0104 - val_mean_absolute_error: 0.0150\n",
      "Epoch 77/300\n",
      "386/386 [==============================] - 0s 47us/sample - loss: 0.0126 - mean_absolute_error: 0.0153 - val_loss: 0.0102 - val_mean_absolute_error: 0.0148\n",
      "Epoch 78/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0123 - mean_absolute_error: 0.0152 - val_loss: 0.0100 - val_mean_absolute_error: 0.0148\n",
      "Epoch 79/300\n",
      "386/386 [==============================] - 0s 40us/sample - loss: 0.0121 - mean_absolute_error: 0.0151 - val_loss: 0.0098 - val_mean_absolute_error: 0.0147\n",
      "Epoch 80/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0118 - mean_absolute_error: 0.0150 - val_loss: 0.0097 - val_mean_absolute_error: 0.0146\n",
      "Epoch 81/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0116 - mean_absolute_error: 0.0148 - val_loss: 0.0095 - val_mean_absolute_error: 0.0145\n",
      "Epoch 82/300\n",
      "386/386 [==============================] - 0s 45us/sample - loss: 0.0114 - mean_absolute_error: 0.0147 - val_loss: 0.0093 - val_mean_absolute_error: 0.0144\n",
      "Epoch 83/300\n",
      "386/386 [==============================] - 0s 44us/sample - loss: 0.0112 - mean_absolute_error: 0.0146 - val_loss: 0.0092 - val_mean_absolute_error: 0.0143\n",
      "Epoch 84/300\n",
      "386/386 [==============================] - 0s 45us/sample - loss: 0.0110 - mean_absolute_error: 0.0145 - val_loss: 0.0090 - val_mean_absolute_error: 0.0142\n",
      "Epoch 85/300\n",
      "386/386 [==============================] - 0s 42us/sample - loss: 0.0107 - mean_absolute_error: 0.0144 - val_loss: 0.0088 - val_mean_absolute_error: 0.0141\n",
      "Epoch 86/300\n",
      "386/386 [==============================] - 0s 44us/sample - loss: 0.0106 - mean_absolute_error: 0.0143 - val_loss: 0.0087 - val_mean_absolute_error: 0.0140\n",
      "Epoch 87/300\n",
      "386/386 [==============================] - 0s 44us/sample - loss: 0.0104 - mean_absolute_error: 0.0142 - val_loss: 0.0085 - val_mean_absolute_error: 0.0140\n",
      "Epoch 88/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0102 - mean_absolute_error: 0.0141 - val_loss: 0.0084 - val_mean_absolute_error: 0.0139\n",
      "Epoch 89/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0100 - mean_absolute_error: 0.0140 - val_loss: 0.0083 - val_mean_absolute_error: 0.0138\n",
      "Epoch 90/300\n",
      "386/386 [==============================] - 0s 43us/sample - loss: 0.0098 - mean_absolute_error: 0.0139 - val_loss: 0.0081 - val_mean_absolute_error: 0.0137\n",
      "Epoch 91/300\n",
      "386/386 [==============================] - 0s 42us/sample - loss: 0.0097 - mean_absolute_error: 0.0138 - val_loss: 0.0080 - val_mean_absolute_error: 0.0136\n",
      "Epoch 92/300\n",
      "386/386 [==============================] - 0s 43us/sample - loss: 0.0095 - mean_absolute_error: 0.0137 - val_loss: 0.0079 - val_mean_absolute_error: 0.0135\n",
      "Epoch 93/300\n",
      "386/386 [==============================] - 0s 44us/sample - loss: 0.0093 - mean_absolute_error: 0.0136 - val_loss: 0.0077 - val_mean_absolute_error: 0.0134\n",
      "Epoch 94/300\n",
      "386/386 [==============================] - 0s 42us/sample - loss: 0.0092 - mean_absolute_error: 0.0135 - val_loss: 0.0076 - val_mean_absolute_error: 0.0134\n",
      "Epoch 95/300\n",
      "386/386 [==============================] - 0s 43us/sample - loss: 0.0090 - mean_absolute_error: 0.0134 - val_loss: 0.0075 - val_mean_absolute_error: 0.0133\n",
      "Epoch 96/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "386/386 [==============================] - 0s 43us/sample - loss: 0.0089 - mean_absolute_error: 0.0134 - val_loss: 0.0074 - val_mean_absolute_error: 0.0132\n",
      "Epoch 97/300\n",
      "386/386 [==============================] - 0s 44us/sample - loss: 0.0087 - mean_absolute_error: 0.0133 - val_loss: 0.0073 - val_mean_absolute_error: 0.0131\n",
      "Epoch 98/300\n",
      "386/386 [==============================] - 0s 44us/sample - loss: 0.0086 - mean_absolute_error: 0.0132 - val_loss: 0.0072 - val_mean_absolute_error: 0.0130\n",
      "Epoch 99/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0084 - mean_absolute_error: 0.0131 - val_loss: 0.0071 - val_mean_absolute_error: 0.0130\n",
      "Epoch 100/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0083 - mean_absolute_error: 0.0130 - val_loss: 0.0070 - val_mean_absolute_error: 0.0129\n",
      "Epoch 101/300\n",
      "386/386 [==============================] - 0s 44us/sample - loss: 0.0082 - mean_absolute_error: 0.0130 - val_loss: 0.0069 - val_mean_absolute_error: 0.0128\n",
      "Epoch 102/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0081 - mean_absolute_error: 0.0129 - val_loss: 0.0068 - val_mean_absolute_error: 0.0128\n",
      "Epoch 103/300\n",
      "386/386 [==============================] - 0s 43us/sample - loss: 0.0079 - mean_absolute_error: 0.0128 - val_loss: 0.0067 - val_mean_absolute_error: 0.0127\n",
      "Epoch 104/300\n",
      "386/386 [==============================] - 0s 44us/sample - loss: 0.0078 - mean_absolute_error: 0.0127 - val_loss: 0.0066 - val_mean_absolute_error: 0.0127\n",
      "Epoch 105/300\n",
      "386/386 [==============================] - 0s 43us/sample - loss: 0.0077 - mean_absolute_error: 0.0127 - val_loss: 0.0065 - val_mean_absolute_error: 0.0126\n",
      "Epoch 106/300\n",
      "386/386 [==============================] - 0s 44us/sample - loss: 0.0076 - mean_absolute_error: 0.0126 - val_loss: 0.0064 - val_mean_absolute_error: 0.0125\n",
      "Epoch 107/300\n",
      "386/386 [==============================] - 0s 42us/sample - loss: 0.0075 - mean_absolute_error: 0.0125 - val_loss: 0.0063 - val_mean_absolute_error: 0.0125\n",
      "Epoch 108/300\n",
      "386/386 [==============================] - 0s 54us/sample - loss: 0.0074 - mean_absolute_error: 0.0125 - val_loss: 0.0062 - val_mean_absolute_error: 0.0124\n",
      "Epoch 109/300\n",
      "386/386 [==============================] - 0s 54us/sample - loss: 0.0073 - mean_absolute_error: 0.0124 - val_loss: 0.0062 - val_mean_absolute_error: 0.0123\n",
      "Epoch 110/300\n",
      "386/386 [==============================] - 0s 45us/sample - loss: 0.0072 - mean_absolute_error: 0.0123 - val_loss: 0.0061 - val_mean_absolute_error: 0.0123\n",
      "Epoch 111/300\n",
      "386/386 [==============================] - 0s 59us/sample - loss: 0.0071 - mean_absolute_error: 0.0123 - val_loss: 0.0060 - val_mean_absolute_error: 0.0122\n",
      "Epoch 112/300\n",
      "386/386 [==============================] - 0s 52us/sample - loss: 0.0070 - mean_absolute_error: 0.0122 - val_loss: 0.0059 - val_mean_absolute_error: 0.0122\n",
      "Epoch 113/300\n",
      "386/386 [==============================] - 0s 54us/sample - loss: 0.0069 - mean_absolute_error: 0.0121 - val_loss: 0.0059 - val_mean_absolute_error: 0.0121\n",
      "Epoch 114/300\n",
      "386/386 [==============================] - 0s 57us/sample - loss: 0.0068 - mean_absolute_error: 0.0121 - val_loss: 0.0058 - val_mean_absolute_error: 0.0120\n",
      "Epoch 115/300\n",
      "386/386 [==============================] - 0s 49us/sample - loss: 0.0067 - mean_absolute_error: 0.0120 - val_loss: 0.0057 - val_mean_absolute_error: 0.0120\n",
      "Epoch 116/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0066 - mean_absolute_error: 0.0119 - val_loss: 0.0056 - val_mean_absolute_error: 0.0119\n",
      "Epoch 117/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0065 - mean_absolute_error: 0.0119 - val_loss: 0.0056 - val_mean_absolute_error: 0.0119\n",
      "Epoch 118/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0064 - mean_absolute_error: 0.0118 - val_loss: 0.0055 - val_mean_absolute_error: 0.0118\n",
      "Epoch 119/300\n",
      "386/386 [==============================] - 0s 43us/sample - loss: 0.0063 - mean_absolute_error: 0.0118 - val_loss: 0.0054 - val_mean_absolute_error: 0.0118\n",
      "Epoch 120/300\n",
      "386/386 [==============================] - 0s 44us/sample - loss: 0.0062 - mean_absolute_error: 0.0117 - val_loss: 0.0054 - val_mean_absolute_error: 0.0117\n",
      "Epoch 121/300\n",
      "386/386 [==============================] - 0s 47us/sample - loss: 0.0061 - mean_absolute_error: 0.0117 - val_loss: 0.0053 - val_mean_absolute_error: 0.0117\n",
      "Epoch 122/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0061 - mean_absolute_error: 0.0116 - val_loss: 0.0053 - val_mean_absolute_error: 0.0116\n",
      "Epoch 123/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0060 - mean_absolute_error: 0.0115 - val_loss: 0.0052 - val_mean_absolute_error: 0.0116\n",
      "Epoch 124/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0059 - mean_absolute_error: 0.0115 - val_loss: 0.0051 - val_mean_absolute_error: 0.0115\n",
      "Epoch 125/300\n",
      "386/386 [==============================] - 0s 44us/sample - loss: 0.0058 - mean_absolute_error: 0.0114 - val_loss: 0.0051 - val_mean_absolute_error: 0.0115\n",
      "Epoch 126/300\n",
      "386/386 [==============================] - 0s 45us/sample - loss: 0.0058 - mean_absolute_error: 0.0114 - val_loss: 0.0050 - val_mean_absolute_error: 0.0114\n",
      "Epoch 127/300\n",
      "386/386 [==============================] - 0s 42us/sample - loss: 0.0057 - mean_absolute_error: 0.0113 - val_loss: 0.0050 - val_mean_absolute_error: 0.0114\n",
      "Epoch 128/300\n",
      "386/386 [==============================] - 0s 44us/sample - loss: 0.0056 - mean_absolute_error: 0.0113 - val_loss: 0.0049 - val_mean_absolute_error: 0.0113\n",
      "Epoch 129/300\n",
      "386/386 [==============================] - ETA: 0s - loss: 0.0064 - mean_absolute_error: 0.012 - 0s 42us/sample - loss: 0.0056 - mean_absolute_error: 0.0112 - val_loss: 0.0049 - val_mean_absolute_error: 0.0113\n",
      "Epoch 130/300\n",
      "386/386 [==============================] - 0s 43us/sample - loss: 0.0055 - mean_absolute_error: 0.0112 - val_loss: 0.0048 - val_mean_absolute_error: 0.0113\n",
      "Epoch 131/300\n",
      "386/386 [==============================] - 0s 44us/sample - loss: 0.0054 - mean_absolute_error: 0.0112 - val_loss: 0.0048 - val_mean_absolute_error: 0.0112\n",
      "Epoch 132/300\n",
      "386/386 [==============================] - 0s 72us/sample - loss: 0.0054 - mean_absolute_error: 0.0111 - val_loss: 0.0047 - val_mean_absolute_error: 0.0111\n",
      "Epoch 133/300\n",
      "386/386 [==============================] - 0s 72us/sample - loss: 0.0053 - mean_absolute_error: 0.0111 - val_loss: 0.0047 - val_mean_absolute_error: 0.0111\n",
      "Epoch 134/300\n",
      "386/386 [==============================] - 0s 44us/sample - loss: 0.0052 - mean_absolute_error: 0.0110 - val_loss: 0.0046 - val_mean_absolute_error: 0.0111\n",
      "Epoch 135/300\n",
      "386/386 [==============================] - 0s 44us/sample - loss: 0.0052 - mean_absolute_error: 0.0110 - val_loss: 0.0046 - val_mean_absolute_error: 0.0110\n",
      "Epoch 136/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0051 - mean_absolute_error: 0.0109 - val_loss: 0.0045 - val_mean_absolute_error: 0.0111\n",
      "Epoch 137/300\n",
      "386/386 [==============================] - 0s 39us/sample - loss: 0.0051 - mean_absolute_error: 0.0109 - val_loss: 0.0045 - val_mean_absolute_error: 0.0110\n",
      "Epoch 138/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0050 - mean_absolute_error: 0.0109 - val_loss: 0.0044 - val_mean_absolute_error: 0.0110\n",
      "Epoch 139/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0050 - mean_absolute_error: 0.0108 - val_loss: 0.0044 - val_mean_absolute_error: 0.0109\n",
      "Epoch 140/300\n",
      "386/386 [==============================] - 0s 44us/sample - loss: 0.0049 - mean_absolute_error: 0.0108 - val_loss: 0.0044 - val_mean_absolute_error: 0.0109\n",
      "Epoch 141/300\n",
      "386/386 [==============================] - 0s 37us/sample - loss: 0.0049 - mean_absolute_error: 0.0107 - val_loss: 0.0043 - val_mean_absolute_error: 0.0108\n",
      "Epoch 142/300\n",
      "386/386 [==============================] - 0s 36us/sample - loss: 0.0048 - mean_absolute_error: 0.0107 - val_loss: 0.0043 - val_mean_absolute_error: 0.0108\n",
      "Epoch 143/300\n",
      "386/386 [==============================] - 0s 38us/sample - loss: 0.0048 - mean_absolute_error: 0.0107 - val_loss: 0.0042 - val_mean_absolute_error: 0.0107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/300\n",
      "386/386 [==============================] - 0s 38us/sample - loss: 0.0047 - mean_absolute_error: 0.0106 - val_loss: 0.0042 - val_mean_absolute_error: 0.0107\n",
      "Epoch 145/300\n",
      "386/386 [==============================] - 0s 36us/sample - loss: 0.0047 - mean_absolute_error: 0.0106 - val_loss: 0.0042 - val_mean_absolute_error: 0.0106\n",
      "Epoch 146/300\n",
      "386/386 [==============================] - 0s 36us/sample - loss: 0.0046 - mean_absolute_error: 0.0106 - val_loss: 0.0041 - val_mean_absolute_error: 0.0106\n",
      "Epoch 147/300\n",
      "386/386 [==============================] - 0s 39us/sample - loss: 0.0046 - mean_absolute_error: 0.0105 - val_loss: 0.0041 - val_mean_absolute_error: 0.0106\n",
      "Epoch 148/300\n",
      "386/386 [==============================] - 0s 35us/sample - loss: 0.0045 - mean_absolute_error: 0.0105 - val_loss: 0.0041 - val_mean_absolute_error: 0.0105\n",
      "Epoch 149/300\n",
      "386/386 [==============================] - 0s 36us/sample - loss: 0.0045 - mean_absolute_error: 0.0104 - val_loss: 0.0040 - val_mean_absolute_error: 0.0105\n",
      "Epoch 150/300\n",
      "386/386 [==============================] - 0s 36us/sample - loss: 0.0044 - mean_absolute_error: 0.0104 - val_loss: 0.0040 - val_mean_absolute_error: 0.0105\n",
      "Epoch 151/300\n",
      "386/386 [==============================] - 0s 35us/sample - loss: 0.0044 - mean_absolute_error: 0.0104 - val_loss: 0.0040 - val_mean_absolute_error: 0.0104\n",
      "Epoch 152/300\n",
      "386/386 [==============================] - 0s 35us/sample - loss: 0.0044 - mean_absolute_error: 0.0103 - val_loss: 0.0039 - val_mean_absolute_error: 0.0104\n",
      "Epoch 153/300\n",
      "386/386 [==============================] - 0s 36us/sample - loss: 0.0043 - mean_absolute_error: 0.0103 - val_loss: 0.0039 - val_mean_absolute_error: 0.0103\n",
      "Epoch 154/300\n",
      "386/386 [==============================] - 0s 37us/sample - loss: 0.0043 - mean_absolute_error: 0.0103 - val_loss: 0.0039 - val_mean_absolute_error: 0.0103\n",
      "Epoch 155/300\n",
      "386/386 [==============================] - 0s 36us/sample - loss: 0.0042 - mean_absolute_error: 0.0102 - val_loss: 0.0038 - val_mean_absolute_error: 0.0103\n",
      "Epoch 156/300\n",
      "386/386 [==============================] - 0s 35us/sample - loss: 0.0042 - mean_absolute_error: 0.0102 - val_loss: 0.0038 - val_mean_absolute_error: 0.0102\n",
      "Epoch 157/300\n",
      "386/386 [==============================] - 0s 36us/sample - loss: 0.0042 - mean_absolute_error: 0.0102 - val_loss: 0.0038 - val_mean_absolute_error: 0.0102\n",
      "Epoch 158/300\n",
      "386/386 [==============================] - 0s 36us/sample - loss: 0.0041 - mean_absolute_error: 0.0101 - val_loss: 0.0037 - val_mean_absolute_error: 0.0101\n",
      "Epoch 159/300\n",
      "386/386 [==============================] - 0s 39us/sample - loss: 0.0041 - mean_absolute_error: 0.0101 - val_loss: 0.0037 - val_mean_absolute_error: 0.0101\n",
      "Epoch 160/300\n",
      "386/386 [==============================] - 0s 37us/sample - loss: 0.0041 - mean_absolute_error: 0.0101 - val_loss: 0.0037 - val_mean_absolute_error: 0.0101\n",
      "Epoch 161/300\n",
      "386/386 [==============================] - 0s 38us/sample - loss: 0.0040 - mean_absolute_error: 0.0101 - val_loss: 0.0037 - val_mean_absolute_error: 0.0102\n",
      "Epoch 162/300\n",
      "386/386 [==============================] - 0s 35us/sample - loss: 0.0040 - mean_absolute_error: 0.0100 - val_loss: 0.0036 - val_mean_absolute_error: 0.0101\n",
      "Epoch 163/300\n",
      "386/386 [==============================] - 0s 35us/sample - loss: 0.0040 - mean_absolute_error: 0.0100 - val_loss: 0.0036 - val_mean_absolute_error: 0.0101\n",
      "Epoch 164/300\n",
      "386/386 [==============================] - 0s 38us/sample - loss: 0.0039 - mean_absolute_error: 0.0100 - val_loss: 0.0036 - val_mean_absolute_error: 0.0100\n",
      "Epoch 165/300\n",
      "386/386 [==============================] - 0s 37us/sample - loss: 0.0039 - mean_absolute_error: 0.0100 - val_loss: 0.0035 - val_mean_absolute_error: 0.0100\n",
      "Epoch 166/300\n",
      "386/386 [==============================] - 0s 37us/sample - loss: 0.0039 - mean_absolute_error: 0.0099 - val_loss: 0.0035 - val_mean_absolute_error: 0.0100\n",
      "Epoch 167/300\n",
      "386/386 [==============================] - 0s 36us/sample - loss: 0.0038 - mean_absolute_error: 0.0099 - val_loss: 0.0035 - val_mean_absolute_error: 0.0099\n",
      "Epoch 168/300\n",
      "386/386 [==============================] - 0s 36us/sample - loss: 0.0038 - mean_absolute_error: 0.0099 - val_loss: 0.0035 - val_mean_absolute_error: 0.0099\n",
      "Epoch 169/300\n",
      "386/386 [==============================] - 0s 38us/sample - loss: 0.0038 - mean_absolute_error: 0.0098 - val_loss: 0.0035 - val_mean_absolute_error: 0.0099\n",
      "Epoch 170/300\n",
      "386/386 [==============================] - 0s 39us/sample - loss: 0.0037 - mean_absolute_error: 0.0098 - val_loss: 0.0034 - val_mean_absolute_error: 0.0099\n",
      "Epoch 171/300\n",
      "386/386 [==============================] - 0s 38us/sample - loss: 0.0037 - mean_absolute_error: 0.0098 - val_loss: 0.0034 - val_mean_absolute_error: 0.0098\n",
      "Epoch 172/300\n",
      "386/386 [==============================] - 0s 38us/sample - loss: 0.0037 - mean_absolute_error: 0.0098 - val_loss: 0.0034 - val_mean_absolute_error: 0.0098\n",
      "Epoch 173/300\n",
      "386/386 [==============================] - 0s 35us/sample - loss: 0.0036 - mean_absolute_error: 0.0097 - val_loss: 0.0033 - val_mean_absolute_error: 0.0097\n",
      "Epoch 174/300\n",
      "386/386 [==============================] - 0s 39us/sample - loss: 0.0036 - mean_absolute_error: 0.0097 - val_loss: 0.0033 - val_mean_absolute_error: 0.0096\n",
      "Epoch 175/300\n",
      "386/386 [==============================] - 0s 38us/sample - loss: 0.0036 - mean_absolute_error: 0.0097 - val_loss: 0.0033 - val_mean_absolute_error: 0.0096\n",
      "Epoch 176/300\n",
      "386/386 [==============================] - 0s 38us/sample - loss: 0.0036 - mean_absolute_error: 0.0096 - val_loss: 0.0033 - val_mean_absolute_error: 0.0096\n",
      "Epoch 177/300\n",
      "386/386 [==============================] - 0s 36us/sample - loss: 0.0035 - mean_absolute_error: 0.0096 - val_loss: 0.0033 - val_mean_absolute_error: 0.0096\n",
      "Epoch 178/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0035 - mean_absolute_error: 0.0096 - val_loss: 0.0032 - val_mean_absolute_error: 0.0096\n",
      "Epoch 179/300\n",
      "386/386 [==============================] - 0s 39us/sample - loss: 0.0035 - mean_absolute_error: 0.0096 - val_loss: 0.0032 - val_mean_absolute_error: 0.0096\n",
      "Epoch 180/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0035 - mean_absolute_error: 0.0096 - val_loss: 0.0032 - val_mean_absolute_error: 0.0095\n",
      "Epoch 181/300\n",
      "386/386 [==============================] - 0s 38us/sample - loss: 0.0034 - mean_absolute_error: 0.0095 - val_loss: 0.0032 - val_mean_absolute_error: 0.0095\n",
      "Epoch 182/300\n",
      "386/386 [==============================] - 0s 38us/sample - loss: 0.0034 - mean_absolute_error: 0.0095 - val_loss: 0.0032 - val_mean_absolute_error: 0.0095\n",
      "Epoch 183/300\n",
      "386/386 [==============================] - 0s 39us/sample - loss: 0.0034 - mean_absolute_error: 0.0095 - val_loss: 0.0032 - val_mean_absolute_error: 0.0095\n",
      "Epoch 184/300\n",
      "386/386 [==============================] - 0s 38us/sample - loss: 0.0034 - mean_absolute_error: 0.0095 - val_loss: 0.0031 - val_mean_absolute_error: 0.0095\n",
      "Epoch 185/300\n",
      "386/386 [==============================] - 0s 36us/sample - loss: 0.0033 - mean_absolute_error: 0.0094 - val_loss: 0.0031 - val_mean_absolute_error: 0.0094\n",
      "Epoch 186/300\n",
      "386/386 [==============================] - 0s 38us/sample - loss: 0.0033 - mean_absolute_error: 0.0094 - val_loss: 0.0031 - val_mean_absolute_error: 0.0094\n",
      "Epoch 187/300\n",
      "386/386 [==============================] - 0s 39us/sample - loss: 0.0033 - mean_absolute_error: 0.0094 - val_loss: 0.0031 - val_mean_absolute_error: 0.0094\n",
      "Epoch 188/300\n",
      "386/386 [==============================] - 0s 39us/sample - loss: 0.0033 - mean_absolute_error: 0.0094 - val_loss: 0.0031 - val_mean_absolute_error: 0.0094\n",
      "Epoch 189/300\n",
      "386/386 [==============================] - 0s 40us/sample - loss: 0.0033 - mean_absolute_error: 0.0094 - val_loss: 0.0031 - val_mean_absolute_error: 0.0094\n",
      "Epoch 190/300\n",
      "386/386 [==============================] - 0s 39us/sample - loss: 0.0032 - mean_absolute_error: 0.0093 - val_loss: 0.0030 - val_mean_absolute_error: 0.0093\n",
      "Epoch 191/300\n",
      "386/386 [==============================] - 0s 40us/sample - loss: 0.0032 - mean_absolute_error: 0.0093 - val_loss: 0.0030 - val_mean_absolute_error: 0.0093\n",
      "Epoch 192/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0032 - mean_absolute_error: 0.0093 - val_loss: 0.0030 - val_mean_absolute_error: 0.0093\n",
      "Epoch 193/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0032 - mean_absolute_error: 0.0093 - val_loss: 0.0030 - val_mean_absolute_error: 0.0092\n",
      "Epoch 194/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0032 - mean_absolute_error: 0.0092 - val_loss: 0.0030 - val_mean_absolute_error: 0.0092\n",
      "Epoch 195/300\n",
      "386/386 [==============================] - 0s 40us/sample - loss: 0.0031 - mean_absolute_error: 0.0092 - val_loss: 0.0030 - val_mean_absolute_error: 0.0092\n",
      "Epoch 196/300\n",
      "386/386 [==============================] - 0s 39us/sample - loss: 0.0031 - mean_absolute_error: 0.0092 - val_loss: 0.0030 - val_mean_absolute_error: 0.0092\n",
      "Epoch 197/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0031 - mean_absolute_error: 0.0092 - val_loss: 0.0029 - val_mean_absolute_error: 0.0092\n",
      "Epoch 198/300\n",
      "386/386 [==============================] - 0s 39us/sample - loss: 0.0031 - mean_absolute_error: 0.0092 - val_loss: 0.0029 - val_mean_absolute_error: 0.0092\n",
      "Epoch 199/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0031 - mean_absolute_error: 0.0091 - val_loss: 0.0029 - val_mean_absolute_error: 0.0091\n",
      "Epoch 200/300\n",
      "386/386 [==============================] - 0s 39us/sample - loss: 0.0030 - mean_absolute_error: 0.0091 - val_loss: 0.0029 - val_mean_absolute_error: 0.0091\n",
      "Epoch 201/300\n",
      "386/386 [==============================] - 0s 36us/sample - loss: 0.0030 - mean_absolute_error: 0.0091 - val_loss: 0.0029 - val_mean_absolute_error: 0.0091\n",
      "Epoch 202/300\n",
      "386/386 [==============================] - 0s 37us/sample - loss: 0.0030 - mean_absolute_error: 0.0091 - val_loss: 0.0029 - val_mean_absolute_error: 0.0091\n",
      "Epoch 203/300\n",
      "386/386 [==============================] - 0s 39us/sample - loss: 0.0030 - mean_absolute_error: 0.0091 - val_loss: 0.0029 - val_mean_absolute_error: 0.0091\n",
      "Epoch 204/300\n",
      "386/386 [==============================] - 0s 37us/sample - loss: 0.0030 - mean_absolute_error: 0.0091 - val_loss: 0.0028 - val_mean_absolute_error: 0.0091\n",
      "Epoch 205/300\n",
      "386/386 [==============================] - 0s 36us/sample - loss: 0.0030 - mean_absolute_error: 0.0090 - val_loss: 0.0028 - val_mean_absolute_error: 0.0091\n",
      "Epoch 206/300\n",
      "386/386 [==============================] - 0s 37us/sample - loss: 0.0029 - mean_absolute_error: 0.0090 - val_loss: 0.0028 - val_mean_absolute_error: 0.0090\n",
      "Epoch 207/300\n",
      "386/386 [==============================] - 0s 36us/sample - loss: 0.0029 - mean_absolute_error: 0.0090 - val_loss: 0.0028 - val_mean_absolute_error: 0.0090\n",
      "Epoch 208/300\n",
      "386/386 [==============================] - 0s 36us/sample - loss: 0.0029 - mean_absolute_error: 0.0090 - val_loss: 0.0028 - val_mean_absolute_error: 0.0090\n",
      "Epoch 209/300\n",
      "386/386 [==============================] - 0s 36us/sample - loss: 0.0029 - mean_absolute_error: 0.0090 - val_loss: 0.0028 - val_mean_absolute_error: 0.0090\n",
      "Epoch 210/300\n",
      "386/386 [==============================] - 0s 37us/sample - loss: 0.0029 - mean_absolute_error: 0.0090 - val_loss: 0.0028 - val_mean_absolute_error: 0.0090\n",
      "Epoch 211/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0029 - mean_absolute_error: 0.0089 - val_loss: 0.0028 - val_mean_absolute_error: 0.0090\n",
      "Epoch 212/300\n",
      "386/386 [==============================] - 0s 35us/sample - loss: 0.0029 - mean_absolute_error: 0.0089 - val_loss: 0.0027 - val_mean_absolute_error: 0.0090\n",
      "Epoch 213/300\n",
      "386/386 [==============================] - 0s 39us/sample - loss: 0.0028 - mean_absolute_error: 0.0089 - val_loss: 0.0027 - val_mean_absolute_error: 0.0089\n",
      "Epoch 214/300\n",
      "386/386 [==============================] - 0s 36us/sample - loss: 0.0028 - mean_absolute_error: 0.0089 - val_loss: 0.0027 - val_mean_absolute_error: 0.0089\n",
      "Epoch 215/300\n",
      "386/386 [==============================] - 0s 35us/sample - loss: 0.0028 - mean_absolute_error: 0.0089 - val_loss: 0.0027 - val_mean_absolute_error: 0.0089\n",
      "Epoch 216/300\n",
      "386/386 [==============================] - 0s 36us/sample - loss: 0.0028 - mean_absolute_error: 0.0089 - val_loss: 0.0027 - val_mean_absolute_error: 0.0089\n",
      "Epoch 217/300\n",
      "386/386 [==============================] - 0s 36us/sample - loss: 0.0028 - mean_absolute_error: 0.0088 - val_loss: 0.0027 - val_mean_absolute_error: 0.0089\n",
      "Epoch 218/300\n",
      "386/386 [==============================] - 0s 39us/sample - loss: 0.0028 - mean_absolute_error: 0.0088 - val_loss: 0.0027 - val_mean_absolute_error: 0.0088\n",
      "Epoch 219/300\n",
      "386/386 [==============================] - 0s 36us/sample - loss: 0.0028 - mean_absolute_error: 0.0088 - val_loss: 0.0027 - val_mean_absolute_error: 0.0088\n",
      "Epoch 220/300\n",
      "386/386 [==============================] - 0s 37us/sample - loss: 0.0027 - mean_absolute_error: 0.0088 - val_loss: 0.0026 - val_mean_absolute_error: 0.0088\n",
      "Epoch 221/300\n",
      "386/386 [==============================] - 0s 34us/sample - loss: 0.0027 - mean_absolute_error: 0.0088 - val_loss: 0.0026 - val_mean_absolute_error: 0.0088\n",
      "Epoch 222/300\n",
      "386/386 [==============================] - 0s 39us/sample - loss: 0.0027 - mean_absolute_error: 0.0088 - val_loss: 0.0026 - val_mean_absolute_error: 0.0088\n",
      "Epoch 223/300\n",
      "386/386 [==============================] - 0s 38us/sample - loss: 0.0027 - mean_absolute_error: 0.0087 - val_loss: 0.0026 - val_mean_absolute_error: 0.0088\n",
      "Epoch 224/300\n",
      "386/386 [==============================] - 0s 36us/sample - loss: 0.0027 - mean_absolute_error: 0.0087 - val_loss: 0.0026 - val_mean_absolute_error: 0.0088\n",
      "Epoch 225/300\n",
      "386/386 [==============================] - 0s 36us/sample - loss: 0.0027 - mean_absolute_error: 0.0087 - val_loss: 0.0026 - val_mean_absolute_error: 0.0088\n",
      "Epoch 226/300\n",
      "386/386 [==============================] - 0s 40us/sample - loss: 0.0027 - mean_absolute_error: 0.0087 - val_loss: 0.0026 - val_mean_absolute_error: 0.0087\n",
      "Epoch 227/300\n",
      "386/386 [==============================] - 0s 36us/sample - loss: 0.0027 - mean_absolute_error: 0.0087 - val_loss: 0.0026 - val_mean_absolute_error: 0.0087\n",
      "Epoch 228/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0026 - mean_absolute_error: 0.0087 - val_loss: 0.0026 - val_mean_absolute_error: 0.0087\n",
      "Epoch 229/300\n",
      "386/386 [==============================] - 0s 36us/sample - loss: 0.0026 - mean_absolute_error: 0.0087 - val_loss: 0.0025 - val_mean_absolute_error: 0.0087\n",
      "Epoch 230/300\n",
      "386/386 [==============================] - 0s 36us/sample - loss: 0.0026 - mean_absolute_error: 0.0087 - val_loss: 0.0025 - val_mean_absolute_error: 0.0087\n",
      "Epoch 231/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0026 - mean_absolute_error: 0.0086 - val_loss: 0.0025 - val_mean_absolute_error: 0.0086\n",
      "Epoch 232/300\n",
      "386/386 [==============================] - 0s 37us/sample - loss: 0.0026 - mean_absolute_error: 0.0086 - val_loss: 0.0025 - val_mean_absolute_error: 0.0086\n",
      "Epoch 233/300\n",
      "386/386 [==============================] - 0s 39us/sample - loss: 0.0026 - mean_absolute_error: 0.0086 - val_loss: 0.0025 - val_mean_absolute_error: 0.0086\n",
      "Epoch 234/300\n",
      "386/386 [==============================] - 0s 36us/sample - loss: 0.0026 - mean_absolute_error: 0.0086 - val_loss: 0.0025 - val_mean_absolute_error: 0.0086\n",
      "Epoch 235/300\n",
      "386/386 [==============================] - 0s 34us/sample - loss: 0.0026 - mean_absolute_error: 0.0086 - val_loss: 0.0025 - val_mean_absolute_error: 0.0086\n",
      "Epoch 236/300\n",
      "386/386 [==============================] - 0s 38us/sample - loss: 0.0026 - mean_absolute_error: 0.0086 - val_loss: 0.0025 - val_mean_absolute_error: 0.0086\n",
      "Epoch 237/300\n",
      "386/386 [==============================] - 0s 37us/sample - loss: 0.0025 - mean_absolute_error: 0.0086 - val_loss: 0.0025 - val_mean_absolute_error: 0.0086\n",
      "Epoch 238/300\n",
      "386/386 [==============================] - 0s 43us/sample - loss: 0.0025 - mean_absolute_error: 0.0085 - val_loss: 0.0025 - val_mean_absolute_error: 0.0086\n",
      "Epoch 239/300\n",
      "386/386 [==============================] - 0s 39us/sample - loss: 0.0025 - mean_absolute_error: 0.0085 - val_loss: 0.0025 - val_mean_absolute_error: 0.0085\n",
      "Epoch 240/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0025 - mean_absolute_error: 0.0085 - val_loss: 0.0025 - val_mean_absolute_error: 0.0086\n",
      "Epoch 241/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0025 - mean_absolute_error: 0.0085 - val_loss: 0.0025 - val_mean_absolute_error: 0.0086\n",
      "Epoch 242/300\n",
      "386/386 [==============================] - 0s 36us/sample - loss: 0.0025 - mean_absolute_error: 0.0085 - val_loss: 0.0024 - val_mean_absolute_error: 0.0085\n",
      "Epoch 243/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0025 - mean_absolute_error: 0.0085 - val_loss: 0.0024 - val_mean_absolute_error: 0.0085\n",
      "Epoch 244/300\n",
      "386/386 [==============================] - 0s 36us/sample - loss: 0.0025 - mean_absolute_error: 0.0085 - val_loss: 0.0024 - val_mean_absolute_error: 0.0086\n",
      "Epoch 245/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0025 - mean_absolute_error: 0.0085 - val_loss: 0.0024 - val_mean_absolute_error: 0.0085\n",
      "Epoch 246/300\n",
      "386/386 [==============================] - 0s 43us/sample - loss: 0.0024 - mean_absolute_error: 0.0085 - val_loss: 0.0024 - val_mean_absolute_error: 0.0085\n",
      "Epoch 247/300\n",
      "386/386 [==============================] - 0s 37us/sample - loss: 0.0024 - mean_absolute_error: 0.0084 - val_loss: 0.0024 - val_mean_absolute_error: 0.0085\n",
      "Epoch 248/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0024 - mean_absolute_error: 0.0084 - val_loss: 0.0024 - val_mean_absolute_error: 0.0085\n",
      "Epoch 249/300\n",
      "386/386 [==============================] - 0s 40us/sample - loss: 0.0024 - mean_absolute_error: 0.0084 - val_loss: 0.0024 - val_mean_absolute_error: 0.0085\n",
      "Epoch 250/300\n",
      "386/386 [==============================] - 0s 38us/sample - loss: 0.0024 - mean_absolute_error: 0.0084 - val_loss: 0.0024 - val_mean_absolute_error: 0.0085\n",
      "Epoch 251/300\n",
      "386/386 [==============================] - 0s 42us/sample - loss: 0.0024 - mean_absolute_error: 0.0084 - val_loss: 0.0024 - val_mean_absolute_error: 0.0085\n",
      "Epoch 252/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0024 - mean_absolute_error: 0.0084 - val_loss: 0.0024 - val_mean_absolute_error: 0.0085\n",
      "Epoch 253/300\n",
      "386/386 [==============================] - 0s 44us/sample - loss: 0.0024 - mean_absolute_error: 0.0084 - val_loss: 0.0024 - val_mean_absolute_error: 0.0084\n",
      "Epoch 254/300\n",
      "386/386 [==============================] - 0s 46us/sample - loss: 0.0024 - mean_absolute_error: 0.0084 - val_loss: 0.0024 - val_mean_absolute_error: 0.0085\n",
      "Epoch 255/300\n",
      "386/386 [==============================] - 0s 44us/sample - loss: 0.0024 - mean_absolute_error: 0.0084 - val_loss: 0.0024 - val_mean_absolute_error: 0.0084\n",
      "Epoch 256/300\n",
      "386/386 [==============================] - 0s 52us/sample - loss: 0.0024 - mean_absolute_error: 0.0083 - val_loss: 0.0023 - val_mean_absolute_error: 0.0084\n",
      "Epoch 257/300\n",
      "386/386 [==============================] - 0s 40us/sample - loss: 0.0024 - mean_absolute_error: 0.0083 - val_loss: 0.0023 - val_mean_absolute_error: 0.0084\n",
      "Epoch 258/300\n",
      "386/386 [==============================] - 0s 43us/sample - loss: 0.0023 - mean_absolute_error: 0.0083 - val_loss: 0.0023 - val_mean_absolute_error: 0.0084\n",
      "Epoch 259/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0023 - mean_absolute_error: 0.0083 - val_loss: 0.0023 - val_mean_absolute_error: 0.0084\n",
      "Epoch 260/300\n",
      "386/386 [==============================] - 0s 39us/sample - loss: 0.0023 - mean_absolute_error: 0.0083 - val_loss: 0.0023 - val_mean_absolute_error: 0.0084\n",
      "Epoch 261/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0023 - mean_absolute_error: 0.0083 - val_loss: 0.0023 - val_mean_absolute_error: 0.0084\n",
      "Epoch 262/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0023 - mean_absolute_error: 0.0083 - val_loss: 0.0023 - val_mean_absolute_error: 0.0084\n",
      "Epoch 263/300\n",
      "386/386 [==============================] - 0s 40us/sample - loss: 0.0023 - mean_absolute_error: 0.0083 - val_loss: 0.0023 - val_mean_absolute_error: 0.0083\n",
      "Epoch 264/300\n",
      "386/386 [==============================] - 0s 39us/sample - loss: 0.0023 - mean_absolute_error: 0.0083 - val_loss: 0.0023 - val_mean_absolute_error: 0.0083\n",
      "Epoch 265/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0023 - mean_absolute_error: 0.0083 - val_loss: 0.0023 - val_mean_absolute_error: 0.0083\n",
      "Epoch 266/300\n",
      "386/386 [==============================] - 0s 46us/sample - loss: 0.0023 - mean_absolute_error: 0.0082 - val_loss: 0.0023 - val_mean_absolute_error: 0.0083\n",
      "Epoch 267/300\n",
      "386/386 [==============================] - 0s 40us/sample - loss: 0.0023 - mean_absolute_error: 0.0082 - val_loss: 0.0023 - val_mean_absolute_error: 0.0083\n",
      "Epoch 268/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0023 - mean_absolute_error: 0.0082 - val_loss: 0.0023 - val_mean_absolute_error: 0.0083\n",
      "Epoch 269/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0023 - mean_absolute_error: 0.0082 - val_loss: 0.0023 - val_mean_absolute_error: 0.0084\n",
      "Epoch 270/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0023 - mean_absolute_error: 0.0082 - val_loss: 0.0023 - val_mean_absolute_error: 0.0083\n",
      "Epoch 271/300\n",
      "386/386 [==============================] - 0s 42us/sample - loss: 0.0022 - mean_absolute_error: 0.0082 - val_loss: 0.0023 - val_mean_absolute_error: 0.0083\n",
      "Epoch 272/300\n",
      "386/386 [==============================] - 0s 39us/sample - loss: 0.0022 - mean_absolute_error: 0.0082 - val_loss: 0.0022 - val_mean_absolute_error: 0.0083\n",
      "Epoch 273/300\n",
      "386/386 [==============================] - 0s 40us/sample - loss: 0.0022 - mean_absolute_error: 0.0082 - val_loss: 0.0022 - val_mean_absolute_error: 0.0083\n",
      "Epoch 274/300\n",
      "386/386 [==============================] - 0s 40us/sample - loss: 0.0022 - mean_absolute_error: 0.0082 - val_loss: 0.0022 - val_mean_absolute_error: 0.0082\n",
      "Epoch 275/300\n",
      "386/386 [==============================] - 0s 40us/sample - loss: 0.0022 - mean_absolute_error: 0.0082 - val_loss: 0.0022 - val_mean_absolute_error: 0.0082\n",
      "Epoch 276/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0022 - mean_absolute_error: 0.0081 - val_loss: 0.0022 - val_mean_absolute_error: 0.0082\n",
      "Epoch 277/300\n",
      "386/386 [==============================] - 0s 39us/sample - loss: 0.0022 - mean_absolute_error: 0.0081 - val_loss: 0.0022 - val_mean_absolute_error: 0.0082\n",
      "Epoch 278/300\n",
      "386/386 [==============================] - 0s 37us/sample - loss: 0.0022 - mean_absolute_error: 0.0081 - val_loss: 0.0022 - val_mean_absolute_error: 0.0082\n",
      "Epoch 279/300\n",
      "386/386 [==============================] - 0s 37us/sample - loss: 0.0022 - mean_absolute_error: 0.0081 - val_loss: 0.0022 - val_mean_absolute_error: 0.0082\n",
      "Epoch 280/300\n",
      "386/386 [==============================] - 0s 39us/sample - loss: 0.0022 - mean_absolute_error: 0.0081 - val_loss: 0.0022 - val_mean_absolute_error: 0.0081\n",
      "Epoch 281/300\n",
      "386/386 [==============================] - 0s 36us/sample - loss: 0.0022 - mean_absolute_error: 0.0081 - val_loss: 0.0022 - val_mean_absolute_error: 0.0081\n",
      "Epoch 282/300\n",
      "386/386 [==============================] - 0s 43us/sample - loss: 0.0022 - mean_absolute_error: 0.0081 - val_loss: 0.0022 - val_mean_absolute_error: 0.0081\n",
      "Epoch 283/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0022 - mean_absolute_error: 0.0081 - val_loss: 0.0022 - val_mean_absolute_error: 0.0081\n",
      "Epoch 284/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0022 - mean_absolute_error: 0.0081 - val_loss: 0.0022 - val_mean_absolute_error: 0.0081\n",
      "Epoch 285/300\n",
      "386/386 [==============================] - 0s 40us/sample - loss: 0.0022 - mean_absolute_error: 0.0081 - val_loss: 0.0022 - val_mean_absolute_error: 0.0081\n",
      "Epoch 286/300\n",
      "386/386 [==============================] - 0s 40us/sample - loss: 0.0022 - mean_absolute_error: 0.0081 - val_loss: 0.0022 - val_mean_absolute_error: 0.0081\n",
      "Epoch 287/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0021 - mean_absolute_error: 0.0080 - val_loss: 0.0022 - val_mean_absolute_error: 0.0081\n",
      "Epoch 288/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0021 - mean_absolute_error: 0.0080 - val_loss: 0.0022 - val_mean_absolute_error: 0.0081\n",
      "Epoch 289/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0021 - mean_absolute_error: 0.0080 - val_loss: 0.0022 - val_mean_absolute_error: 0.0081\n",
      "Epoch 290/300\n",
      "386/386 [==============================] - 0s 39us/sample - loss: 0.0021 - mean_absolute_error: 0.0080 - val_loss: 0.0021 - val_mean_absolute_error: 0.0080\n",
      "Epoch 291/300\n",
      "386/386 [==============================] - 0s 37us/sample - loss: 0.0021 - mean_absolute_error: 0.0080 - val_loss: 0.0021 - val_mean_absolute_error: 0.0080\n",
      "Epoch 292/300\n",
      "386/386 [==============================] - 0s 41us/sample - loss: 0.0021 - mean_absolute_error: 0.0080 - val_loss: 0.0021 - val_mean_absolute_error: 0.0080\n",
      "Epoch 293/300\n",
      "386/386 [==============================] - 0s 38us/sample - loss: 0.0021 - mean_absolute_error: 0.0080 - val_loss: 0.0021 - val_mean_absolute_error: 0.0080\n",
      "Epoch 294/300\n",
      "386/386 [==============================] - 0s 39us/sample - loss: 0.0021 - mean_absolute_error: 0.0080 - val_loss: 0.0021 - val_mean_absolute_error: 0.0080\n",
      "Epoch 295/300\n",
      "386/386 [==============================] - 0s 38us/sample - loss: 0.0021 - mean_absolute_error: 0.0080 - val_loss: 0.0021 - val_mean_absolute_error: 0.0080\n",
      "Epoch 296/300\n",
      "386/386 [==============================] - 0s 39us/sample - loss: 0.0021 - mean_absolute_error: 0.0080 - val_loss: 0.0021 - val_mean_absolute_error: 0.0080\n",
      "Epoch 297/300\n",
      "386/386 [==============================] - 0s 36us/sample - loss: 0.0021 - mean_absolute_error: 0.0080 - val_loss: 0.0021 - val_mean_absolute_error: 0.0080\n",
      "Epoch 298/300\n",
      "386/386 [==============================] - 0s 37us/sample - loss: 0.0021 - mean_absolute_error: 0.0080 - val_loss: 0.0021 - val_mean_absolute_error: 0.0080\n",
      "Epoch 299/300\n",
      "386/386 [==============================] - 0s 38us/sample - loss: 0.0021 - mean_absolute_error: 0.0079 - val_loss: 0.0021 - val_mean_absolute_error: 0.0080\n",
      "Epoch 300/300\n",
      "386/386 [==============================] - 0s 36us/sample - loss: 0.0021 - mean_absolute_error: 0.0079 - val_loss: 0.0021 - val_mean_absolute_error: 0.0080\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_data, y_data, validation_split=0.2, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12,5))\n",
    "ax[0].plot(history.history['loss'], label='train')\n",
    "ax[0].plot(history.history['val_loss'], label='test')\n",
    "ax[0].set_title('Loss')\n",
    "ax[0].legend()\n",
    "ax[1].plot(history.history['mean_absolute_error'], label='train')\n",
    "ax[1].plot(history.history['val_mean_absolute_error'], label='test')\n",
    "ax[1].set_title('Accuracy')\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
